{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **16-Systems of Differential Equations**\n",
    "\n",
    "---\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "This notebook goes over the basics of systems of differential equations. \n",
    "\n",
    "---\n",
    "\n",
    "### **Author**\n",
    "**Junichi Koganemaru**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Last Updated**\n",
    "**March 9, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up till this point we have been studying scalar equations, which are equations for which the range of the solution is a subset of $\\mathbb{R}$. \n",
    "\n",
    "Recall that in the introduction to second order equations, we went over an example converting a scalar second order equation into a vector-valued first order system. This turns out to be a generic feature of ordinary differential equations: any scalar ordinary differential equation can be converted into an equivalent vector-valued first order system. \n",
    "\n",
    "As a result, instead of studying higher order scalar equations in an ad hoc way by considering third order, fourth order, etc. equations, we can instead study them systematically by converting them into vector-valued first order systems and then studying the properties of these systems. \n",
    "\n",
    "In this notebook we focus on $2 \\times 2$ systems for the sake of simplicity, but the generalization to $n \\times n$ systems is straightforward.\n",
    "\n",
    "In order to work within this framework, we first need to introduce some preliminary concepts from linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites from linear algebra\n",
    "\n",
    "### Vectors and matrices\n",
    "A vector $\\boldsymbol{v} \\in \\mathbb{R}^n$ is a tuple of $n$ real numbers, which we commonly record as inside a column: \n",
    "$$\n",
    "    \\boldsymbol{v} =  \\begin{pmatrix}\n",
    "        v_1  \\\\\n",
    "        \\vdots \\\\\n",
    "        v_n\n",
    "    \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "A matrix $A \\in \\mathcal{M}_{m \\times n}(\\mathbb{R})$ is a collection of $m \\times n$ real numbers, which we commonly in an array of $m$ rows and $n$ columns: \n",
    "$$\n",
    "       A = \\begin{pmatrix} \n",
    "       a_{11} & a_{12} & \\ldots & a_{1n} \\\\\n",
    "       a_{21} & a_{22} & \\ldots & a_{2n} \\\\\n",
    "       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       a_{m1} & a_{m2} & \\ldots & a_{mn}\n",
    "     \\end{pmatrix}.\n",
    "$$\n",
    "The element in the $i$-th row and $j$-th column of a matrix is commonly denoted as $A_{ij}$ or $(A)_{ij}$. \n",
    "\n",
    "To make our lives easier, we will focus exclusively on the case when $m = n = 2$. Given \n",
    "$$\n",
    "       \\boldsymbol{v} = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}, \\quad \\boldsymbol{w} = \\begin{pmatrix} w_1 \\\\ w_2 \\end{pmatrix},\n",
    "$$ \n",
    "we define the sum of the $\\boldsymbol{v},\\boldsymbol{w}$ as another vector, denoted by $\\boldsymbol{v} + \\boldsymbol{w}$, where its entries are defined via\n",
    "$$\n",
    "     \\boldsymbol{v} + \\boldsymbol{w} := \\begin{pmatrix} v_1 + w_1 \\\\ v_2 + w_2 \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "Let $A, B \\in \\mathcal{M}_{2 \\times 2}(\\mathbb{R})$. The **sum** of $A,B$, denote by $A+B$, is another matrix in $\\mathcal{M}_{2 \\times 2}(\\mathbb{R})$ where its entries are given by \n",
    "$$\n",
    "      (A+B)_{ij} = (A)_{ij} + (B)_{ij} \\; \\text{for all} \\; 1 \\le i \\le 2, 1 \\le j \\le 2.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Scalar-vector and matrix-vector multiplication\n",
    "The scalar-vector product $c \\boldsymbol{v}$ between a scalar $c \\in \\mathbb{R}$ and a vector $\\boldsymbol{v} \\in \\mathbb{R}^2$ is a vector in $\\mathbb{R}^2$ defined via \n",
    "$$\n",
    "    c \\boldsymbol{v} = c \\begin{pmatrix}\n",
    "        v_1  \\\\\n",
    "        v_2\n",
    "    \\end{pmatrix} = \\begin{pmatrix}\n",
    "        c v_1  \\\\\n",
    "        c v_2\n",
    "    \\end{pmatrix}.\n",
    "$$   \n",
    "\n",
    "The matrix-vector product $A \\boldsymbol{v}$ between a $2 \\times 2$ matrix $A$ and a vector $\\boldsymbol{v} \\in \\mathbb{R}^2$ is defined via \n",
    "$$\n",
    "    A \\boldsymbol{v} = \\begin{pmatrix}\n",
    "        a_{11} & a_{12}\\\\\n",
    "        a_{21} & a_{22}\n",
    "    \\end{pmatrix} \\begin{pmatrix}\n",
    "        v_1  \\\\\n",
    "        v_2\n",
    "    \\end{pmatrix} = v_1 \\begin{pmatrix}\n",
    "        a_{11}  \\\\\n",
    "        a_{21}\n",
    "    \\end{pmatrix} + v_2 \\begin{pmatrix}\n",
    "        a_{12}  \\\\\n",
    "        a_{22}\n",
    "    \\end{pmatrix} = \\begin{pmatrix}\n",
    "        v_1 a_{11} + v_2 a_{12}  \\\\\n",
    "        v_1 a_{21} + v_2 a_{22}\n",
    "    \\end{pmatrix}.\n",
    "$$  \n",
    "Note that in general, $A \\boldsymbol{v} = B \\boldsymbol{v}$ for a vector $\\boldsymbol{v} \\in \\mathbb{R}^2$ does not imply $A = B$.    \n",
    "\n",
    "Let $A \\in \\mathcal{M}_{2 \\times 2}(\\mathbb{R})$ and let $c$ be any real number. The **scalar multiple of factor $c$** of $A$, denote by $cA$, is another matrix in $\\mathcal{M}_{2 \\times 2}(\\mathbb{R})$ where its entries are given by \n",
    "$$\n",
    "         (cA)_{ij} = c(A)_{ij} \\; \\text{for all} \\; 1 \\le i \\le 2, 1 \\le j \\le 2.\n",
    "$$\n",
    "\n",
    "\n",
    "### Matrix-matrix multiplication \n",
    "Note that if $B$ is a $2 \\times 2$ matrix and $\\boldsymbol{v} \\in \\mathbb{R}^2$, then $B \\boldsymbol{v}$ is a vector in $\\mathbb{R}^2$. If $A$ is another $2 \\times 2$ matrix, then the matrix-vector product $A(B \\boldsymbol{v})$ is well-defined. A natural question to ask is that given $A,B \\in \\mathcal{M}_{2 \\times 2}(\\mathbb{R})$, is it possible to compute $A(B \\boldsymbol{v})$ \"in one shot\" without first computing $B \\boldsymbol{v}$ and then computing $A(B \\boldsymbol{v})$? This motivates the definition of **matrix-matrix multiplication**. \n",
    "\n",
    "\n",
    "If $A,B$ are two $2 \\times 2$ matrices, then the matrix-matrix product $AB$ is a $2 \\times 2$ matrix defined via \n",
    "$$\n",
    "           A B = A \\begin{pmatrix}\n",
    "               \\boldsymbol{b}_1   & \n",
    "             \\boldsymbol{b}_2 \n",
    "           \\end{pmatrix}  = \\begin{pmatrix}\n",
    "              A \\boldsymbol{b}_1  &  A \\boldsymbol{b}_2 \n",
    "           \\end{pmatrix}.\n",
    "$$\n",
    "More explicitly, \n",
    "$$\n",
    "            (AB)_{ij} = (A)_{i1} (B)_{1j} + (A)_{i2}(B)_{2j} \\; \\text{for all} \\; 1 \\le i \\le 2, 1 \\le j \\le 2.\n",
    "$$\n",
    "\n",
    "One can show that by defining matrix-matrix multiplication in this way, we have $(AB) \\boldsymbol{v} = A(B \\boldsymbol{v})$ for all $\\boldsymbol{v} \\in \\mathbb{R}^2$.\n",
    "\n",
    "**Remark:** Note that in general, $AB \\neq BA$. \n",
    "\n",
    "\n",
    "While we defined matrix-matrix multiplication \"column-wise\", one can also define it \"entry-wise\" or \"row-wise\". Below we list three equivalent ways of defining matrix-matrix multiplication.\n",
    "\n",
    "* Entry wise: the element in the $i$-th row and the $j$-th column of $AB$ is formed by using elements in the $i$-row of $A$ and the $j$-th column of $B$. \n",
    "* Column wise: the $j$-th column of $AB$ is formed by using elements of the $j$-th column of $B$ specifying how to combine the columns of $A$.\n",
    "* Row wise: the $i$-th row of $AB$ is formed by using elements of the $i$-th row of $A$ specifying how to combine the rows of $B$. \n",
    "\n",
    "> **Example**\n",
    "> Suppose we want to compute \n",
    "> $$\n",
    "> AB =  \\begin{pmatrix} 1 & 2 \\\\\n",
    "> 3 & 4\\end{pmatrix} \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}.\n",
    "> $$\n",
    "> We can compute this element wise:\n",
    "> $$\n",
    "> \\begin{pmatrix} 1 & 2 \\\\\n",
    "> 3 & 4\\end{pmatrix} \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix} = \\begin{pmatrix} 1 \\times 5 + 2 \\times 7 & 1 \\times 6 + 2 \\times 8 \\\\ 3 \\times 5 + 4 \\times 7 & 3 \\times 6 + 4 \\times 8  \\end{pmatrix} \n",
    "> = \\begin{pmatrix}\n",
    "> 19 & 22 \\\\\n",
    "> 43 & 50\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> \n",
    "> We can also compute this row wise. The first row of $AB$ is \n",
    "> $$\n",
    "> \\begin{pmatrix}\n",
    "> 1 & 2 \n",
    "> \\end{pmatrix} \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix} = 1 \\begin{pmatrix}\n",
    "> 5&6\n",
    "> \\end{pmatrix} + 2 \\begin{pmatrix}\n",
    "> 7&8\n",
    "> \\end{pmatrix} \n",
    "> = \\begin{pmatrix}\n",
    "> 5&6\n",
    "> \\end{pmatrix} + \\begin{pmatrix}\n",
    "> 14 & 16\n",
    "> \\end{pmatrix} = \\begin{pmatrix}\n",
    "> 19 & 22\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> \n",
    "> The second row of $AB$ is \n",
    "> $$\n",
    "> \\begin{pmatrix}\n",
    "> 3 & 4 \n",
    "> \\end{pmatrix} \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix} = 3 \\begin{pmatrix}\n",
    "> 5&6\n",
    "> \\end{pmatrix} + 4 \\begin{pmatrix}\n",
    "> 7&8\n",
    "> \\end{pmatrix} \n",
    "> = \\begin{pmatrix}\n",
    "> 15&18\n",
    "> \\end{pmatrix} + \\begin{pmatrix}\n",
    "> 28 & 32\n",
    "> \\end{pmatrix} = \\begin{pmatrix}\n",
    "> 43 & 50\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> \n",
    "> We can also compute this column wise. The first column of $AB$ is \n",
    "> $$\n",
    "> \\begin{pmatrix} 1 & 2 \\\\\n",
    "> 3 & 4\\end{pmatrix} \\begin{pmatrix} 5  \\\\ 7 \\end{pmatrix} = 5 \\begin{pmatrix}\n",
    "> 1  \\\\\n",
    "> 3 \n",
    "> \\end{pmatrix}  + 7 \\begin{pmatrix}\n",
    "> 2  \\\\\n",
    "> 4 \n",
    "> \\end{pmatrix} \n",
    "> = \\begin{pmatrix}\n",
    "> 5 \\\\\n",
    "> 15\n",
    "> \\end{pmatrix} + \\begin{pmatrix}\n",
    "> 14 \\\\ 28\n",
    "> \\end{pmatrix} = \\begin{pmatrix}\n",
    "> 19 \\\\\n",
    "> 43\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> \n",
    "> The second column of $AB$ is \n",
    "> $$\n",
    "> \\begin{pmatrix} 1 & 2 \\\\\n",
    "> 3 & 4\\end{pmatrix} \\begin{pmatrix} 6\\\\ 8 \\end{pmatrix} = 6 \\begin{pmatrix}\n",
    "> 1  \\\\\n",
    "> 3 \n",
    "> \\end{pmatrix}  + 8 \\begin{pmatrix}\n",
    "> 2  \\\\\n",
    "> 4 \n",
    "> \\end{pmatrix}  \n",
    "> = \\begin{pmatrix}\n",
    "> 6 \\\\\n",
    "> 18\n",
    "> \\end{pmatrix} + \\begin{pmatrix}\n",
    "> 16 \\\\ 32\n",
    "> \\end{pmatrix} = \\begin{pmatrix}\n",
    "> 22 \\\\\n",
    "> 50\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "\n",
    "**Remark:** For practical computational problems, try to compute $AB$ either row by row or column by column. Computing them element-wise is typically too slow.\n",
    "\n",
    "\n",
    "\n",
    "We note that matrix-matrix multiplication differs fundamentally from scalar-scalar multiplication in that the order in which the multiplication is performed matters. We can see this explicitly via the following example.\n",
    "\n",
    "> **Example (Noncommutativity of matrix multiplication)**:\n",
    "> Non-commutativity is a fancy way of saying $AB$ doesn't necessarily equal $BA$. Here is a concrete example. Let \n",
    "> $$\n",
    "> A = \\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\end{pmatrix}, \\quad B = \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\end{pmatrix}.\n",
    "> $$\n",
    "> Then \n",
    "> $$\n",
    "> AB = \\begin{pmatrix} 19 & 22 \\\\ 43 & 50 \\end{pmatrix},\n",
    "> $$\n",
    "> yet\n",
    "> $$\n",
    "> BA = \\begin{pmatrix} 23 & 34 \\\\ 31 & 46 \\end{pmatrix}.\n",
    "> $$\n",
    "\n",
    "### The identity matrix\n",
    "The $2 \\times 2$ identity matrix is the matrix \n",
    "$$\n",
    "    I_{2 \\times 2} = \\begin{pmatrix}\n",
    "        1 & 0  \\\\\n",
    "        0 & 1\n",
    "    \\end{pmatrix}. \n",
    "$$\n",
    "It satisfies the identity \n",
    "$$\n",
    "    I\\boldsymbol{v} = \\boldsymbol{v} \\; \\text{for all} \\; \\boldsymbol{v} \\in \\mathbb{R}^2     \n",
    "$$\n",
    "and \n",
    "$$\n",
    "    I A = A I = A \\; \\text{for all} \\; A \\in \\mathcal{M}_{2 \\times 2}(\\mathbb{R}).\n",
    "$$\n",
    "\n",
    "### Determinants and invertible matrices\n",
    "A $2 \\times 2$ matrix is said to be invertible if there exists another $2 \\times 2$ matrix, commonly denoted as $A^{-1}$, such that \n",
    "$$\n",
    "    A A^{-1} = A^{-1} A = I_{2 \\times 2}.\n",
    "$$\n",
    "\n",
    "The determinant of a $2 \\times 2$ matrix is a scalar defined via \n",
    "$$\n",
    "    \\det \\begin{pmatrix}\n",
    "        a & b \\\\\n",
    "        c & d\n",
    "    \\end{pmatrix} = ad - bc.\n",
    "$$\n",
    "\n",
    "One can show that a matrix $A$ is invertible if and only if $\\det A \\neq 0$. If $\\det A \\neq 0$, then \n",
    "$$\n",
    "        \\begin{pmatrix}\n",
    "           a & b  \\\\\n",
    "           c & d\n",
    "        \\end{pmatrix}^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix}\n",
    "           d & -b \\\\\n",
    "           -c & a\n",
    "        \\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "### Eigenvalues and eigenvectors\n",
    "A (possibly complex) scalar $\\lambda$ is said to be an eigenvalue of a $2 \\times 2$ matrix $A$ if $A - \\lambda I$ is not invertible. This is equivalent to requiring that $\\lambda$ is a root of the **characteristic polynomial**\n",
    "$$\n",
    "    p_A(\\lambda) = \\det (A - \\lambda I), \\; \\lambda \\in \\mathbb{C},\n",
    "$$\n",
    "which is a quadratic polynomial in $\\lambda$. \n",
    "\n",
    "If $\\lambda$ is an eigenvalue of $A$, then there exists a non-zero vector $\\boldsymbol{v} \\in \\mathbb{R}^2$ such that $A \\boldsymbol{v} = \\lambda \\boldsymbol{v}$ or equivalently, $(A - \\lambda I)\\boldsymbol{v} = \\boldsymbol{0}$. Any such vector is referred to as an eigenvector of $A$ associated to the eigenvalue $\\lambda$.   \n",
    "\n",
    "Note that if $c$ is a non-zero constant, then \n",
    "$$\n",
    "    A \\boldsymbol{v} = \\lambda \\boldsymbol{v} \\Longleftrightarrow A(c \\boldsymbol{v} ) = \\lambda (c \\boldsymbol{v} ), \\quad c \\in \\mathbb{R}.\n",
    "$$\n",
    "This means that eigenvectors are not unique: if $\\boldsymbol{v}$ is an eigenvector, then so is any non-zero constant multiple of $\\boldsymbol{v}$. This has two implications. \n",
    "1. You always have a choice when you try to look for eigenvectors.\n",
    "2. For $2 \\times 2$ systems, there will be always be redundancy in the system, so you only need to use one of the two equations. \n",
    "\n",
    "Here are a few nice facts from linear algebra.\n",
    "* The determinant is the product of the eigenvalues.\n",
    "* The trace (sum of the elements on the main diagonal) is the sum of the eigenvalues.\n",
    "* If $A+\\lambda I$ has rows or columns that are multiples of each other, then $-\\lambda$ is an eigenvalue. A special case of this is when $\\lambda = 0$.\n",
    "* If $A$ is a triangular matrix, meaning that the elements above (or below) the main diagonal are all zeros, then the eigenvalues are the entries on the main diagonal.\n",
    "* Eigenvectors corresponding to distinct eigenvalues are linearly independent.\n",
    "\n",
    "> **Example**\n",
    "> Suppose \n",
    "> $$\n",
    ">      A = \\begin{pmatrix}\n",
    ">          1 & 1 \\\\\n",
    ">          1 & 1\n",
    ">      \\end{pmatrix}.\n",
    "> $$\n",
    "> Note that $A$ has two rows that are multiples of each other, so it has $\\lambda_1 = 0$ as an eigenvalue. Since the trace of $A$ is $1 + 1 = 2$, the other eigenvalue is $\\lambda_2 = 2$. \n",
    "\n",
    "> **Example**\n",
    "> Suppose \n",
    "> $$\n",
    ">      A = \\begin{pmatrix}\n",
    ">          1 & 3 \\\\\n",
    ">          0 & 2\n",
    ">      \\end{pmatrix}.\n",
    "> $$\n",
    "> Here $A$ is a triangular matrix, so its eigenvalues are $\\lambda_1= 1$ and $\\lambda_2 = 2$.\n",
    "\n",
    "It turns out by studying the so-called *matrix exponential* $e^{A} \\in \\mathcal{M}_{n \\times n}(\\mathbb{R})$ of a matrix $A \\in \\mathcal{M}_{n \\times n}(\\mathbb{R})$, the fundamental set of solutions for constant coefficient systems can be recovered by finding the eigenvalues and eigenvectors of $A$, just like how in the scalar case the fundamental solutions depend on the roots to the characteristic polynomial. We will make this connection precise later in this notebook. \n",
    "\n",
    "### Some calculus with vector and matrix-valued functions\n",
    "Let $I$ be an interval. A function $\\boldsymbol{f} : I \\to \\mathbb{R}^2$ is said to be a vector-valued function if for every $t \\in I, \\boldsymbol{f}(t)$ is a vector in $\\mathbb{R}^2$. One can think of vector-valued functions as function having two components:\n",
    "$$\n",
    "    \\boldsymbol{f}(t) = \\begin{pmatrix}\n",
    "        g(t)  \\\\\n",
    "        h(t)\n",
    "    \\end{pmatrix}, \\; t \\in I\n",
    "$$\n",
    "for two scalar functions $g,h: I \\to \\mathbb{R}$. If $g,h$ are both differentiable over $I$, then the derivative of $\\boldsymbol{f}$ is defined via \n",
    "$$\n",
    "    \\boldsymbol{f}'(t) = \\begin{pmatrix}\n",
    "        g'(t)  \\\\\n",
    "        h'(t)\n",
    "    \\end{pmatrix}, \\; t \\in I.\n",
    "$$ \n",
    "\n",
    "The definite integral of $\\boldsymbol{f}$ for $a,b \\in I$ is defined via \n",
    "$$\n",
    "    \\int_a^b f(t) \\; dt = \\begin{pmatrix}\n",
    "        \\int_a^b g(t) \\; dt \\\\\n",
    "        \\int_a^b h(t) \\; dt\n",
    "    \\end{pmatrix}.\n",
    "$$ \n",
    "\n",
    "Analogously, we can define a matrix-valued function $A: I \\to \\mathcal{M}_{2 \\times 2}(\\mathbb{R})$ to be a function with \"four components\":\n",
    "$$\n",
    "    A(t) = \\begin{pmatrix}\n",
    "        a(t) & b(t)  \\\\\n",
    "        c(t)  & d(t)\n",
    "    \\end{pmatrix}, \\; t \\in I\n",
    "$$ \n",
    "and define the derivatives and antiderivatives of $A$ component wise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systems of differential equations\n",
    "\n",
    "Now we turn our attention to studying systems of differential equations in the form\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_1'(t) = f_1(t,y_1(t), \\ldots, y_n(t)), \\; t \\in I \\\\\n",
    "\\quad \\quad \\quad \\vdots \\\\\n",
    "y_n'(t) = f_n(t,y_1(t),\\ldots,y_n(t)), \\; t \\in I\n",
    "\\end{cases}\n",
    "$$\n",
    "where $I$ is an interval. In this class we'll focus on the study of autonomous systems, which are systems for which the RHS does not depend on the independent variable. These are systems of the form\n",
    "$$\n",
    "\\begin{cases}\n",
    "y_1'(t) = g_1(y_1(t), \\ldots, y_n(t)), \\; t \\in I \\\\\n",
    "\\quad \\quad \\quad \\vdots \\\\\n",
    "y_n'(t) = g_n(y_1(t),\\ldots,y_n(t)), \\; t \\in I\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Systems of differential equations using vector notation\n",
    "\n",
    "In vector notation, this is equivalent to writing $\\boldsymbol{Y}' = \\boldsymbol{g}(\\boldsymbol{Y})$, where $\\boldsymbol{Y}: I \\to \\mathbb{R}^n$ and $\\boldsymbol{g}: \\mathbb{R}^n \\to \\mathbb{R}^n$ are vector-valued functions defined via\n",
    "$$\n",
    "\\boldsymbol{Y}(t) = \\begin{pmatrix} \n",
    "y_1(t) \\\\\n",
    "\\vdots \\\\\n",
    "y_n(t)\n",
    "\\end{pmatrix}, \\quad \\boldsymbol{g}(\\boldsymbol{Y}) = \\begin{pmatrix} \n",
    "g_1(\\boldsymbol{Y}) \\\\\n",
    " \\vdots \\\\\n",
    "g_n(\\boldsymbol{Y})\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    ">**Example**\n",
    "> In mathematical epidemiology, the simplest model used to model the dynamic of an epidemic is the SIR model. In this model, the general population is compartmentalized into three groups:\n",
    "> \n",
    "> - Susceptible: these are individuals who can acquire the disease the next day.\n",
    "> - Infected: these are individuals who have contracted the disease.\n",
    "> - Removed: these are individuals who were able to recover and develop an immunity, or individuals who have passed away from the disease.\n",
    "> The dynamics of this model is governed by a system of nonlinear ODEs, given by\n",
    "> $$\n",
    "> \\begin{cases}\n",
    "> \\frac{\\partial S(t)}{\\partial t} = -\\frac{\\beta I(t) S(t)}{N}, \\; t \\in I \\\\\n",
    "> \\frac{\\partial I(t)}{\\partial t} = \\frac{\\beta I(t) S(t)}{N} - \\gamma I(t), \\; t \\in I \\\\\n",
    "> \\frac{\\partial R(t)}{\\partial t} = \\gamma I(t), \\; t \\in I\n",
    "> \\end{cases}\n",
    "> $$\n",
    "> Here $N$ is the total population. Assuming that the total population doesn't change, i.e. $S(t) + I(t) + R(t) = N = \\; \\text{constant}$, one can in fact reduce the system of three equations to a planar system. This means that the subsequent techniques that we'll develop to analyze planar systems will directly apply to this model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-vector form of 2D linear autonomous systems\n",
    "\n",
    "For the sake of simplicity we will exclusively study $2 \\times 2$ systems. Given a $2 \\times 2$ system of differential equations \n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "x'(t) = a(t) x(t) + b(t) y(t), \\; t \\in I \\\\\n",
    "y'(t) = c(t) x(t) + d(t) y(t), \\; t \\in I\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "the corresponding matrix-vector form of the system is \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "x'(t) \\\\ y'(t) \n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "a(t) & b(t)  \\\\\n",
    "c(t) & d(t)\n",
    "\\end{pmatrix}\\begin{pmatrix}\n",
    "x(t) \\\\ y(t) \n",
    "\\end{pmatrix}, \\; t \\in I.\n",
    "$$\n",
    "\n",
    "If we define the vector-valued function \n",
    "$$\n",
    "\\boldsymbol{X}(t) := \\begin{pmatrix}\n",
    "x(t) \\\\ y(t)\n",
    "\\end{pmatrix}, \\; t \\in I\n",
    "$$\n",
    "then the matrix-vector form is equivalent to \n",
    "$$\n",
    "\\boldsymbol{X}'(t) = A(t) \\boldsymbol{X}(t), \\; t \\in I\n",
    "$$\n",
    "where the time-dependent matrix $A(t)$ is given by \n",
    "$$\n",
    "A(t) = \\begin{pmatrix}\n",
    "a(t) & b(t) \\\\\n",
    "c(t) & d(t)\n",
    "\\end{pmatrix}, \\; t \\in I.\n",
    "$$\n",
    "\n",
    "This is a first-order, vector-valued differential equation in 2D. A special case of this is when $A$ is a constant coefficient matrix, in which case we can write down solutions formulas for the general solution explicitly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## A fundamental reduction\n",
    "\n",
    "Now that we have the notation and terminology down, we can reduce the study of higher order scalar equations to the study of first order vector-valued systems. First we will go over a simple example to illustrate this point.\n",
    "\n",
    "> **Example**\n",
    "> \n",
    "> Consider the scalar IVP given by \n",
    "> $$\n",
    "> \\begin{cases}\n",
    "> y''(t) + y'(t) +  y(t) = 0, \\; t \\in \\mathbb{R} \\\\\n",
    "> y(0) = y'(0) = 1 \n",
    "> \\end{cases}\n",
    "> $$\n",
    "> We show that we can write this in the form of a system of differential equations. Define $Y: \\mathbb{R} \\to \\mathbb{R}^2$ via\n",
    "> $$\n",
    "> \\boldsymbol{Y}(t) = \\begin{pmatrix}\n",
    "> y(t) \\\\\n",
    "> y'(t) \n",
    "> \\end{pmatrix}, \\; t \\in \\mathbb{R}\n",
    "> $$\n",
    "> and notice that by using $y''(t) = -y'(t) - y(t)$, we have \n",
    "> $$\n",
    "> \\begin{cases}\n",
    "> \\boldsymbol{Y}'(t) = \\begin{pmatrix} y'(t) \\\\ y''(t)  \\end{pmatrix}= \\begin{pmatrix} 0 \\cdot y(t) + 1 \\cdot y'(t) \\\\ -1 \\cdot y(t) -1 \\cdot y'(t)  \\end{pmatrix}= \\begin{pmatrix} \n",
    "> 0 &1 \\\\\n",
    "> -1 & -1\n",
    "> \\end{pmatrix} \\begin{pmatrix} y(t) \\\\ y'(t) \\end{pmatrix} = \\begin{pmatrix} \n",
    "> 0 &1 \\\\\n",
    "> -1 & -1\n",
    "> \\end{pmatrix} \\boldsymbol{Y}(t), \\; t \\in \\mathbb{R} \\\\\n",
    "> \\boldsymbol{Y}(0) = \\begin{pmatrix}\n",
    "> y(0) \\\\\n",
    "> y'(0)\n",
    "> \\end{pmatrix} = \\begin{pmatrix}\n",
    "> 1 \\\\ 1\n",
    "> \\end{pmatrix}.\n",
    "> \\end{cases}\n",
    "> $$\n",
    "\n",
    "**Remark:** What we have shown in the previous example is that we can always write a scalar, 2nd order IVP, into a vector-valued, 1st order IVP in 2D. If the scalar-equation is a constant coefficient linear equation, then the corresponding linear system is a 1st order linear system that can be written in the form of $\\boldsymbol{Y}' = A \\boldsymbol{Y}$ where $A$ is a constant coefficient matrix.   \n",
    "\n",
    "Next we consider the general 2nd order constant coefficient linear system \n",
    "$$\n",
    "y''(t) + a y'(t) + by(t) = 0, \\; t \\in \\mathbb{R}.\n",
    "$$\n",
    "We learned that we can write down solution formulas by examining the roots of the characteristic equation\n",
    "$$\n",
    "r^2 + ar + b = 0, \\; r \\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "If we write the 2nd order constant coefficient linear system as a first ordered system by defining \n",
    "$$\n",
    "\\boldsymbol{Y}(t) = \\begin{pmatrix}\n",
    "y(t)  \\\\\n",
    "y'(t)\n",
    "\\end{pmatrix}, \\; t \\in \\mathbb{R}\n",
    "$$\n",
    "Then \n",
    "$$\n",
    "\\boldsymbol{Y}'(t) = \\begin{pmatrix}\n",
    "0 \\cdot y(t) + 1 \\cdot y'(t)  \\\\\n",
    "-by(t) -ay'(t)\n",
    "\\end{pmatrix}  = \\begin{pmatrix}\n",
    "0 & 1 \\\\\n",
    "-b & -a\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "y(t) \\\\\n",
    "y'(t)\n",
    "\\end{pmatrix}=  A \\boldsymbol{Y}(t), \\; t \\in \\mathbb{R}\n",
    "$$\n",
    "for \n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "0 & 1 \\\\\n",
    "-b & -a\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "We note that the characteristic polynomial of $A$ is \n",
    "$$\n",
    "\\det (A - \\lambda I) = \\det \\begin{pmatrix}\n",
    "-\\lambda &1  \\\\\n",
    "-b & -a - \\lambda\n",
    "\\end{pmatrix} = \\lambda(\\lambda+a) + b = \\lambda^2 + a\\lambda + b, \\; \\lambda \\in \\mathbb{R}.\n",
    "$$\n",
    "Here we see that the characteristic polynomial of the matrix $A$ is exactly the characteristic equation associated to the original 2nd order scalar equation. \n",
    "\n",
    "This suggests that the eigenvalues (and by association, the corresponding eigenvectors) of the matrix $A$ should intricately related to the solution to the underlying linear system.\n",
    "\n",
    "## Structure of solution to linear systems\n",
    "\n",
    "\n",
    "We will focus mostly on linear systems of the form \n",
    "$$\n",
    "\\boldsymbol{y}'(t) - A(t) \\boldsymbol{y}(t) = \\boldsymbol{f}(t), \\; t \\in \\mathbb{R}.\n",
    "$$\n",
    "What we will see below is that the structure of solutions to linear systems will closely mirror the structure of solutions to their scalar counterparts. \n",
    "\n",
    "\n",
    "> **Theorem (Existence and uniqueness)**\n",
    "> \n",
    "> Let $I$ be an interval, and let $\\boldsymbol{f}:I \\to \\mathbb{R}^2$ and $A: I \\to \\mathcal{M}_{2 \\times 2}(\\mathbb{R})$ be continuous over $I$. Then there exists a unique solution $\\boldsymbol{y}: I \\to \\mathbb{R}^2$ to the initial value problem \n",
    "> $$\n",
    "> \\begin{cases} \n",
    "> \\boldsymbol{y}'(t) - A(t) \\boldsymbol{y}(t) = \\boldsymbol{f}(t), \\; t \\in \\mathbb{R} \\\\\n",
    "> \\boldsymbol{y}(t_0) = \\boldsymbol{y}_0 \n",
    "> \\end{cases}\n",
    "> $$\n",
    "> for any $t_0 \\in I, \\boldsymbol{y}_0 \\in \\mathbb{R}^2$, and the maximal interval of existence of the solution $\\boldsymbol{y}$ is the interval $I$.  \n",
    "\n",
    "### Fundamental set of solutions\n",
    "\n",
    "Recall that for scalar equations, we've shown that the general solution to a homogeneous, $n$-th order equation can be written as \n",
    "$$\n",
    "y(t) = c_1 y_1(t) + ... + c_n y_n(t), \\; t \\in \\mathbb{R},\n",
    "$$\n",
    "where $y_1(t), ..., y_n(t)$ are linearly independent. We say that $y_1(t), ..., y_n(t)$ form a fundamental set of solutions to the equation.\n",
    "    \n",
    "Analogously, the vector-valued equation \n",
    "$$\n",
    "\\boldsymbol{X}'(t) = A(t) \\boldsymbol{X}(t), \\; t \\in \\mathbb{R},\n",
    "$$\n",
    "in dimension $n$ also admits a fundamental set of solutions. In other words, the general solution to the system can be written as \n",
    "$$\n",
    "\\boldsymbol{X}(t) = c_1 \\boldsymbol{X}_1(t) + ... + c_n \\boldsymbol{X}_n(t), \\; t \\in \\mathbb{R},\n",
    "$$\n",
    "where $\\boldsymbol{X}_1(t), ..., \\boldsymbol{X}_n(t)$ are linearly independent. The linear dependence of these functions is encoded in the Wronskian $W(\\boldsymbol{X}_1(t), ..., \\boldsymbol{X}_n(t))$, where \n",
    "$$\n",
    "W(\\boldsymbol{X}_1(t), ..., \\boldsymbol{X}_n(t)) = \\det \\bigg (\\begin{array}{c|c|c}\n",
    "\\boldsymbol{X}_1(t) &\\cdots &\\boldsymbol{X}_n(t)\n",
    "\\end{array}\\bigg ), \\; t \\in \\mathbb{R}.\n",
    "$$\n",
    "\n",
    "### Linear independence and Wronskian for vector-valued functions\n",
    "\n",
    "Linear independence of vector-valued functions are defined analogous to scalar-valued functions: a set of functions $\\boldsymbol{X}_1(t), \\ldots , \\boldsymbol{X}_n(t)$ are linearly independent over an interval $I$ if \n",
    "$$\n",
    "c_1 \\boldsymbol{X}_1(t) + ... + c_n \\boldsymbol{X}_n(t) = \\boldsymbol{0} \\; \\text{for all} \\; t \\in I \\implies c_1 = \\ldots c_n = 0. \n",
    "$$  \n",
    "Under appropriate conditions on the set of functions, one can show that $\\boldsymbol{X}_1(t), \\ldots , \\boldsymbol{X}_n(t)$ are linearly independent over the interval $I$ if and only if the Wronskian \n",
    "$$\n",
    "W(\\boldsymbol{X}_1(t), ..., \\boldsymbol{X}_n(t))(t) = \\det \\bigg (\\begin{array}{c|c|c}\n",
    "\\boldsymbol{X}_1(t) &\\cdots &\\boldsymbol{X}_n(t)\n",
    "\\end{array}\\bigg ) \\neq 0 \\; \\text{for some} \\; t \\in I.\n",
    "$$\n",
    "\n",
    "\n",
    "For an inhomogeneous system of the form \n",
    "$$\n",
    "\\boldsymbol{X}'(t) - A(t) \\boldsymbol{X}(t) = \\boldsymbol{f}(t), \\; t \\in I,   \n",
    "$$\n",
    "the general solution is \n",
    "$$\n",
    "\\boldsymbol{X}(t) = \\boldsymbol{X}_h(t) + \\boldsymbol{X}_p(t), \\; t \\in I,  \n",
    "$$\n",
    "where $\\boldsymbol{X}_h(t)$ is the general homogeneous solution and $\\boldsymbol{X}_p(t)$ is any particular solution to the system.  \n",
    "\n",
    "## Explicit solution formulas\n",
    "\n",
    "Analogous to 2nd order constant coefficient linear equations, we can write down explicit solution formulas for constant coefficient $2 \\times 2$ systems by identifying the eigenvalues and a corresponding set of eigenvectors for the coefficient matrix $A$.\n",
    "\n",
    "We have three cases to consider for a $2 \\times 2$ matrix.\n",
    "- When $A$ admits two distinct real eigenvalues.\n",
    "- When $A$ admits a repeating real eigenvalue.\n",
    "- When $A$ admits a pair of complex eigenvalues. \n",
    "\n",
    "### Distinct real eigenvalues\n",
    "\n",
    "If $A$ admits two distinct eigenvalues $\\lambda_1 \\neq \\lambda_2$, then the general solution to $\\boldsymbol{X}'(t) = A \\boldsymbol{X}(t)$ over an interval $I$ is \n",
    "$$\n",
    "\\boldsymbol{X}(t) = c_1 e^{\\lambda_1 t} \\boldsymbol{v}_1 + c_2 e^{\\lambda_2 t} \\boldsymbol{v}_2, \\; t \\in I\n",
    "$$\n",
    "where $\\boldsymbol{v}_1, \\boldsymbol{v}_2$ is a set of linearly independent eigenvectors corresponding to $\\lambda_1$ and $\\lambda_2$.   \n",
    "\n",
    "> **Example**\n",
    "> \n",
    "> Consider the system $\\boldsymbol{y}'(t) = A \\boldsymbol{y}(t)$ for \n",
    "> $$\n",
    "> A = \\begin{pmatrix}\n",
    "> 2 & 2\\\\\n",
    "> 1 & 3\n",
    "> \\end{pmatrix}, \\; t \\in \\mathbb{R}.\n",
    "> $$  \n",
    "> The characteristic polynomial of $A$ is given by \n",
    "> $$\n",
    "> \\det(A- \\lambda I) = \\det  \\begin{pmatrix}\n",
    "> 2 - \\lambda& 2\\\\\n",
    "> 1 & 3-\\lambda\n",
    "> \\end{pmatrix} =  (2-\\lambda)(3-\\lambda) - 2 = (\\lambda-4) (\\lambda-1), \\; \\lambda \\in \\mathbb{R}.\n",
    "> $$\n",
    "> Therefore its eigenvalues are $\\lambda_1 = 4$ and $\\lambda_2 = 1$. Now we look for the corresponding eigenvectors. By definition, the eigenvector $\\boldsymbol{v}$ corresponding to $\\lambda_1 = 4$ satisfies $A \\boldsymbol{v} = 4 \\boldsymbol{v}$, or $(A - 4 I) \\boldsymbol{v} = 0$. This is equivalent to looking for a solution to \n",
    "> $$\n",
    "> \\begin{pmatrix}\n",
    "> -2 & 2 \\\\\n",
    "> 1 & -1 \n",
    "> \\end{pmatrix} \\begin{pmatrix}\n",
    "> v_1 \\\\\n",
    "> v_2\n",
    "> \\end{pmatrix} = \\begin{pmatrix}\n",
    "> 0 \\\\\n",
    "> 0\n",
    "> \\end{pmatrix}, \\; t \\in \\mathbb{R}.\n",
    "> $$\n",
    "> Equivalently we'd like to find solutions to \n",
    "> $$\n",
    "> \\begin{cases}\n",
    "> -2 v_1 + 2v_2 = 0\\\\\n",
    "> v_1 - v_2 = 0.\n",
    "> \\end{cases}\n",
    "> $$\n",
    "> We see that any vector with $v_1 = v_2$ satisfies the system above. Thus we can take $\\boldsymbol{v} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n",
    "> For $\\lambda_2 = 1$ we proceed similarly and see that the eigenvector $\\boldsymbol{v}$ satisfies \n",
    "> $$\n",
    "> \\begin{cases}\n",
    "> v_1 + 2v_2 = 0\\\\\n",
    "> v_1 + 2v_2 = 0\n",
    "> \\end{cases}\n",
    "> $$\n",
    "> Thus any vector $\\boldsymbol{v}$ with $v_1 = - 2v_2$ is an eigenvector, so it suffices to take $\\boldsymbol{v} =  \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$. Thus the general solution is given by \n",
    "> $$\n",
    "> \\boldsymbol{y}(t) = c_1 e^{4t}  \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + c_2 e^{t}  \\begin{pmatrix} -2 \\\\1 \\end{pmatrix} = \\begin{pmatrix} c_1 e^{4t} - 2c_2 e^t \\\\ c_1 e^{4t} + c_2 e^{t}\n",
    "> \\end{pmatrix}, \\; t \\in \\mathbb{R}\n",
    "> $$\n",
    "> where $c_1, c_2$ are arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated real eigenvalues\n",
    "\n",
    "If $A$ admits $\\lambda$ as a repeating eigenvalue, and one can find two linearly independent eigenvectors $\\boldsymbol{v}_1, \\boldsymbol{v}_2$ associated to $\\lambda$, then the solution formula remains the same:\n",
    "$$\n",
    "\\boldsymbol{y}(t) = c_1 e^{\\lambda t} \\boldsymbol{v}_1 + c_2 e^{\\lambda t} \\boldsymbol{v}_2.\n",
    "$$\n",
    "\n",
    "> **Example**\n",
    "> Consider the system $\\boldsymbol{y}'(t) = A \\boldsymbol{y}(t), \\; t \\in \\mathbb{R}$ for \n",
    "> $$\n",
    "> A = \\begin{pmatrix}\n",
    "> 1 & 0 \\\\\n",
    "> 0 & 1\n",
    "> \\end{pmatrix}.\n",
    "> $$  \n",
    "> Its eigenvalues are $\\lambda_1 = 1, \\lambda_2 = 1$, so $A$ admits $\\lambda=1$ as a repeating eigenvalue. We note that \n",
    "> $$\n",
    "> (A-I)\\boldsymbol{v} = \\begin{pmatrix}\n",
    "> 0 & 0 \\\\\n",
    "> 0 & 0\n",
    "> \\end{pmatrix} \\begin{pmatrix}\n",
    "> v_1  \\\\\n",
    "> v_2\n",
    "> \\end{pmatrix} = \\begin{pmatrix}\n",
    "> 0   \\\\\n",
    "> 0 \n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> So we can find two linearly independent eigenvectors \n",
    "> $$\n",
    "> \\boldsymbol{v}_1 = \\begin{pmatrix}\n",
    "> 1   \\\\\n",
    "> 0 \n",
    "> \\end{pmatrix}, \\; \\boldsymbol{v}_2 = \\begin{pmatrix}\n",
    "> 0  \\\\\n",
    "> 1 \n",
    "> \\end{pmatrix}.   \n",
    "> $$\n",
    "> Therefore the general solution is \n",
    "> $$\n",
    "> \\boldsymbol{y}(t) = c_1 e^{t} \\begin{pmatrix}\n",
    "> 1  \\\\\n",
    "> 0 \n",
    "> \\end{pmatrix} + c_2 e^{t} \\begin{pmatrix}\n",
    "> 0  \\\\\n",
    "> 1 \n",
    "> \\end{pmatrix} = \\begin{pmatrix}\n",
    "> c_1 e^t  \\\\\n",
    "> c_2 e^t\n",
    "> \\end{pmatrix}, \\; t \\in \\mathbb{R}\n",
    "> $$\n",
    "> where $c_1, c_2$ are arbitrary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized eigenvectors\n",
    "\n",
    "It's not always the case that one can find two linearly independent eigenvectors associated to a repeating eigenvalue $\\lambda$. In this case we need the notion of a *generalized eigenvector*. If $\\boldsymbol{v}$ is an eigenvector associated to $\\lambda$, then the vector $\\boldsymbol{w}$ such that \n",
    "$$\n",
    "(A - \\lambda I) \\boldsymbol{w} = \\boldsymbol{v}\n",
    "$$\n",
    "is called a generalized eigenvector (this is equivalent to requiring $(A - \\lambda I)^2 \\boldsymbol{w} = 0$).\n",
    "\n",
    "In this case the solution formula is \n",
    "$$\n",
    "\\boldsymbol{y}(t) = c_1 e^{\\lambda t} \\boldsymbol{v} + c_2  ( te^{\\lambda t} \\boldsymbol{v} + e^{\\lambda t} \\boldsymbol{w}) = c_1 e^{\\lambda t} \\boldsymbol{v} + c_2  e^{\\lambda t}( t \\boldsymbol{v} + \\boldsymbol{w}), \\quad t \\in \\mathbb{R}.\n",
    "$$\n",
    "Notice the similarity with the solution formula in the scalar case.\n",
    "\n",
    "\n",
    "**Remark:** The reason why we see the $t$ (and powers of $t$) in these formulas has to do with the *Jordan canonical form* of the matrix $A$. We will discuss this in more detail when we go over the matrix exponential more carefully. \n",
    "\n",
    "> **Example**\n",
    "> \n",
    "> Consider the system $\\boldsymbol{y}'(t) = A \\boldsymbol{y}(t)$ for \n",
    "> $$\n",
    "> A = \\begin{pmatrix}\n",
    "> 1 & 1\\\\\n",
    "> 0 & 1\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> Here $\\lambda = 1$ is also a repeating eigenvalue.\n",
    "> An eigenvector $\\boldsymbol{v}$ satisfies $(A- I) \\boldsymbol{v} = 0$ or equivalently, \n",
    "> $$\n",
    "> \\begin{pmatrix}\n",
    "> 0 & 1 \\\\\n",
    "> 0 & 0 \n",
    "> \\end{pmatrix} \\begin{pmatrix} \n",
    "> v_1 \\\\ v_2\n",
    "> \\end{pmatrix} = \\begin{pmatrix}\n",
    "> 0 \\\\\n",
    "> 0\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> We see $\\boldsymbol{v}$ is a solution as long as $v_2 = 0$, so we can choose $\\boldsymbol{v} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Notice that any other eigenvector is a scalar multiple of this $\\boldsymbol{v}$, thus there are no other eigenvectors that are linearly independent of $\\boldsymbol{v}$. In this case we need to look for a *generalized eigenvector* $\\boldsymbol{w}$ satisfying $(A-I)\\boldsymbol{w} = \\boldsymbol{v}$, or  \n",
    "> $$\n",
    "> \\begin{pmatrix}\n",
    "> 0 & 1 \\\\\n",
    "> 0 & 0 \n",
    "> \\end{pmatrix} \\begin{pmatrix} \n",
    "> w_1 \\\\ w_2\n",
    "> \\end{pmatrix}  = \\begin{pmatrix}\n",
    "> 1 \\\\\n",
    "> 0\n",
    "> \\end{pmatrix}\n",
    "> $$\n",
    "> We see that $\\boldsymbol{w}$ is a generalized eigenvector as long we choose $w_2 = 1$, so we can choose $\\boldsymbol{w} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Therefore the general solution is\n",
    "> $$\n",
    "> \\boldsymbol{y}(t) = c_1 e^{t} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_2 \\left( t e^{t} \\begin{pmatrix} 1 \\\\  0\\end{pmatrix}  + e^{t} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}  \\right)= \\begin{pmatrix} c_1 e^t + c_2 te^t \\\\ c_2  e^t \\end{pmatrix}, \\quad t \\in \\mathbb{R}\n",
    "> $$\n",
    "> where $c_1, c_2$ are arbitrary.\n",
    "\n",
    "### Complex eigenvalues\n",
    "\n",
    "If $A$ admits complex eigenvalues $\\lambda_1, \\lambda_2$, then one can show that \n",
    "- We must have $\\lambda_1 = \\overline{\\lambda_2}$, meaning that they are complex conjugates. More explicitly, if $\\lambda_1 = \\alpha + i \\beta$ for $\\alpha,\\beta \\in \\mathbb{R}$, then $\\lambda_2 = \\alpha - i \\beta$. \n",
    "- If $\\boldsymbol{v} = \\boldsymbol{B}_1 + i \\boldsymbol{B}_2$ is an eigenvector for $\\lambda_1 = \\alpha + i \\beta$, then $\\overline{\\boldsymbol{v}} = \\boldsymbol{B}_1 - i \\boldsymbol{B}_2$ is an eigenvector for $\\lambda_2 = \\alpha - i \\beta$. \n",
    "\n",
    "The upshot is that everything comes in pairs in the case when $A$ admits complex eigenvalues, and as a result we only really need to identify one eigenvalue and one eigenvector in order to write down the general solution.\n",
    "\n",
    "If $\\lambda$ is an eigenvalue and $\\boldsymbol{p}$ is an eigenvector of the form  \n",
    "$$\n",
    "\\lambda = \\alpha + i \\beta, \\quad \\boldsymbol{p} = \\boldsymbol{B}_1 + i \\boldsymbol{B}_2,\n",
    "$$\n",
    "then the (real) general solution is given by\n",
    "$$\n",
    "\\boldsymbol{y}(t) = c_1 \\boldsymbol{y}_1(t) + c_2 \\boldsymbol{y}_2(t), \\quad t \\in \\mathbb{R},\n",
    "$$\n",
    "where $c_1,c_2$ are arbitrary and \n",
    "$$\n",
    "\\boldsymbol{y}_1(t) = e^{\\alpha t} \\left[ \\boldsymbol{B}_1 \\cos \\beta t - \\boldsymbol{B}_2 \\sin \\beta t \\right], \\quad t \\in \\mathbb{R}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{y}_2(t) = e^{\\alpha t} \\left[ \\boldsymbol{B}_2 \\cos \\beta t + \\boldsymbol{B}_1 \\sin \\beta t \\right], \\quad t \\in \\mathbb{R}\n",
    "$$\n",
    "\n",
    "### Alternative form of the solution using Euler's identity\n",
    "\n",
    "Using Euler's formula \n",
    "$$\n",
    "e^{i \\theta} = \\cos \\theta + i \\sin \\theta, \\quad \\theta \\in \\mathbb{R}\n",
    "$$\n",
    "and symmetry properties of $\\cos, \\sin$,\n",
    "$$\n",
    "\\cos (-\\theta) = \\cos \\theta, \\quad \\sin(-\\theta) = - \\sin \\theta, \\quad \\theta \\in \\mathbb{R}\n",
    "$$\n",
    "one can also write the general solution as \n",
    "$$\n",
    "\\boldsymbol{X}(t) = c_1 \\Re( \\boldsymbol{p} e^{\\lambda t} ) + c_2 \\Im (\\boldsymbol{p} e^{\\lambda t}), \\quad t \\in \\mathbb{R},\n",
    "$$\n",
    "where $c_1,c_2$ are arbitrary and $\\Re(z), \\Im(z)$ denote the real and imaginary parts of a complex number $z$. \n",
    "\n",
    "> **Example**\n",
    "> Consider the system\n",
    "> $$\n",
    "> \\begin{cases}\n",
    "> x'(t) = x(t) + 2y(t)\\\\\n",
    "> y'(t) = -5x(t) - y(t), \\quad t \\in \\mathbb{R}.\n",
    "> \\end{cases}\n",
    "> $$\n",
    "> In matrix notation this is the system \n",
    "> $$\n",
    "> \\boldsymbol{X}'(t) = A \\boldsymbol{X}(t) = \\begin{pmatrix}\n",
    "> 1 & 2  \\\\\n",
    "> -5 & -1\n",
    "> \\end{pmatrix}  \\boldsymbol{X}(t), \\quad t \\in \\mathbb{R}.\n",
    "> $$\n",
    "> First we look for the eigenvalues of the matrix $\n",
    "> A = \\begin{pmatrix}\n",
    "> 1 & 2 \\\\\n",
    "> -5 & -1\n",
    "> \\end{pmatrix}$. The eigenvalues are the roots to\n",
    "> $$\n",
    "> \\det( A - \\lambda I ) = (1-\\lambda)(-1- \\lambda) + 10 = \\lambda^2 + 9  \\implies \\lambda_1 = 3i, \\lambda_2 =-3i.\n",
    "> $$\n",
    "> Next we look for (complex) eigenvectors corresponding to the eigenvalues (since they come in conjugate pairs we only need to do the computation once). The eigenvector $\\boldsymbol{p}$ corresponding to $\\lambda_1 = 3i$ satisfies $(A - \\lambda_1 I) \\boldsymbol{p} = 0$, so we look for solutions to\n",
    "> $$\n",
    "> \\begin{pmatrix}\n",
    "> 1 - 3i & 2 \\\\\n",
    "> -5 & -1 - 3i \n",
    "> \\end{pmatrix} \\begin{pmatrix}\n",
    "> p_1 \\\\\n",
    "> p_2\n",
    "> \\end{pmatrix} = \\begin{pmatrix}\n",
    "> 0 \\\\\n",
    "> 0\n",
    "> \\end{pmatrix} \\quad \\text{or} \\quad \\begin{cases}\n",
    "> (1-3i) p_1 + 2 p_2 = 0 \\\\\n",
    "> -5 p_1 + (-1-3i) p_2 = 0.\n",
    "> \\end{cases}\n",
    "> $$\n",
    "> From the theory we know that the two equations are redundant, so anything satisfying the first equation is fine. \n",
    "> We see that $\\boldsymbol{p}$ is an eigenvector as long as $5 p_1 + (1+3i)p_2 = 0$, so it suffices to choose $\\boldsymbol{p} = \\begin{pmatrix} - \\frac{1+3i}{5} \\\\ 1 \\end{pmatrix}$ (it's better to choose $p_2 = 1$ since you'll need to calculate what $\\frac{1}{1+3i}$ is the other way around). To write down the (real) general solution we need to write $\\boldsymbol{p}$ as \n",
    "> $$\n",
    "> \\boldsymbol{p} = \\begin{pmatrix} -\\frac{1}{5} \\\\ 1 \\end{pmatrix} + i \\begin{pmatrix} -\\frac{3}{5} \\\\ 0 \\end{pmatrix}.\n",
    "> $$\n",
    "> In this problem, the general solution is given by \n",
    "> $$\n",
    "> \\boldsymbol{X}(t) = c_1 \\left[  \\begin{pmatrix} -\\frac{1}{5} \\\\ 1 \\end{pmatrix}  \\cos( 3 t) - \\begin{pmatrix} -\\frac{3}{5} \\\\ 0 \\end{pmatrix} \\sin(3t)  \\right]  + c_2 \\left[  \\begin{pmatrix} -\\frac{3}{5} \\\\ 0 \\end{pmatrix} \\cos(3 t) +\\begin{pmatrix} -\\frac{1}{5} \\\\ 1 \\end{pmatrix} \\sin(3t) \\right], \\quad t \\in \\mathbb{R}.\n",
    "> $$\n",
    "> One can also calculate\n",
    "> $$\n",
    "> \\boldsymbol{p} e^{\\lambda t} =\\left[ \\begin{pmatrix} -\\frac{1}{5} \\\\ 1 \\end{pmatrix} + i \\begin{pmatrix} -\\frac{3}{5} \\\\ 0 \\end{pmatrix}\\right] (\\cos 3t + i \\sin 3t ) = \\left[ \\begin{pmatrix} -\\frac{1}{5} \\\\ 1 \\end{pmatrix}  \\cos 3t - \\begin{pmatrix} -\\frac{3}{5} \\\\ 0 \\end{pmatrix} \\sin 3t \\right] + i \\left[   \\begin{pmatrix} -\\frac{3}{5} \\\\ 0 \\end{pmatrix} \\cos 3t +  \\begin{pmatrix} -\\frac{1}{5} \\\\ 1 \\end{pmatrix} \\sin 3t\\right], \\quad t \\in \\mathbb{R}\n",
    "> $$\n",
    "> and use the real and complex parts to write down the (real) general solution.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
