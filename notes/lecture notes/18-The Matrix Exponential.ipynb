{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **18-The Matrix Exponential**\n",
    "\n",
    "---\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "In this notebook we study general $n \\times n$ systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Author**\n",
    "**Junichi Koganemaru**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Last Updated**\n",
    "**April 9, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we study general $n \\times n$ constant coefficient systems. To be specific, we want to consider equations of the form $\\boldsymbol{X}'(t) = \\boldsymbol{A} \\boldsymbol{X}(t)$, where $\\boldsymbol{A}$ is a constant $n \\times n$ matrix and $\\boldsymbol{X}: I \\to \\mathbb{R}^n$ is a vector-valued function. Previously we studied the case when $n = 2$, in this notebook we discuss the theory for general $\\mathbb{N} \\ni n > 2$. \n",
    "\n",
    "First, we will go over the definition of matrix-vector multiplication and matrix-matrix multiplication. \n",
    "\n",
    "### Matrix-vector and matrix-matrix multiplication\n",
    "\n",
    "> **Definition (Matrix-vector multiplication):**  \n",
    "> Let $A$ be an $m \\times n$ matrix of the form  \n",
    "> $$  \n",
    "> A = \\begin{pmatrix} \n",
    "> a_{11} & a_{12} & \\ldots & a_{1n} \\\\ \n",
    "> a_{21} & a_{22} & \\ldots & a_{2n} \\\\ \n",
    "> \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "> a_{m1} & a_{m2} & \\ldots & a_{mn} \n",
    "> \\end{pmatrix},  \n",
    "> $$  \n",
    "> and let $\\boldsymbol{x}$ be an $n \\times 1$ vector of the form  \n",
    "> $$  \n",
    "> \\boldsymbol{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix}.  \n",
    "> $$  \n",
    "> Then $A\\boldsymbol{x}$ is defined to be a vector such that its entry in the $i$-th row is given by  \n",
    "> $$  \n",
    "> (A\\boldsymbol{x})_i = a_{i1}\\,x_1 + a_{i2}\\,x_2 + \\ldots + a_{in}\\,x_n.  \n",
    "> $$  \n",
    "\n",
    "To visualize this, focus on the $i$-th row:\n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "a_{i1} & a_{i2} & \\ldots & a_{in} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \n",
    "=\n",
    "\\begin{pmatrix} \n",
    "\\vdots \\\\\n",
    "a_{i1}\\,x_1 + a_{i2}\\,x_2 + \\ldots + a_{in}\\,x_n \\\\\n",
    "\\vdots \n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "**Remark:**  Note that we need  \n",
    "$$\n",
    "\\text{the number of columns in } A = \\text{the number of rows in } \\boldsymbol{x},\n",
    "$$  \n",
    "otherwise this definition breaks down. In our original notation, each row has as many coefficients as there are variables. If these two numbers don't match up, we simply say that the matrix-vector product $A\\boldsymbol{x}$ is undefined or that the components are incompatible.\n",
    "\n",
    "An alternative way to think about this is in terms of the \"column picture.\"\n",
    "\n",
    "> **Proposition** \n",
    "> Let $A \\in \\mathcal{M}_{m \\times n}(\\mathbb{R})$ and let $\\boldsymbol{v} \\in \\mathbb{R}^n$. Suppose $(\\boldsymbol{v})_i = v_i$, and think of the $i$-th column of $A$ as a vector in $\\mathbb{R}^n$ denoted by $\\boldsymbol{a}_i$ for all $1 \\le i \\le n$. Then  \n",
    "> \\begin{align}  \n",
    "> A \\boldsymbol{v} = v_1 \\boldsymbol{a}_1 + \\ldots + v_n \\boldsymbol{a}_n.  \n",
    "> \\end{align}  \n",
    "\n",
    "\n",
    "In other words, the entries of the vector $\\boldsymbol{v}$ specify how to combine the columns of the matrix $A$ to form the matrix-vector product $A \\boldsymbol{v}$.  \n",
    "\n",
    "To visualize this, write  \n",
    "\n",
    "$$\\begin{pmatrix} \\boldsymbol{a_1} & \\mid & \\boldsymbol{a_2} & \\mid & \\ldots & \\mid & \\boldsymbol{a_n} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{pmatrix} = v_1 \\boldsymbol{a_1} + \\ldots + v_n \\boldsymbol{a_n}.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition:** Let $A \\in \\mathcal{M}_{m \\times n}(\\mathbb{R}), B \\in \\mathcal{M}_{n \\times p}(\\mathbb{R})$. The *matrix-matrix product* (or simply the matrix product) between $A,B$, denoted by $AB$, is a matrix in $\\mathcal{M}_{m \\times p}(\\mathbb{R})$ for which its entries are given by  \n",
    "> $$  \n",
    "> (AB)_{ij} = (A)_{i1} (B)_{1j} + (A)_{i2}(B)_{2j} + \\ldots + (A)_{in}B_{nj}  = \\sum_{k=1}^n (A)_{ik}(B)_{kj}, \\; \\text{for all} \\; 1 \\le i \\le m, 1 \\le j \\le p.  \n",
    "> $$  \n",
    "\n",
    "A few remarks before we move on.  \n",
    "\n",
    "**Remark:** $k$ is a dummy variable in the equation above, while $i,j$ are not as they refer to the position of the entry we want to focus on.  \n",
    "\n",
    "**Remark:** Note that there's a compatibility condition for a matrix-matrix product to be well-defined: the number of columns of the first matrix must match the number of rows of the second matrix.  \n",
    "\n",
    "**Remark:**  While this definition is useful for proving identities, it is too slow to be applied for performing computations.  \n",
    "\n",
    "To visualize what we've written down, focus on the $i$-th row of $A$ and the $j$-th column of $B$. This gives us the element in the $i$-th row and $j$-th column of $AB$:  \n",
    "$$  \n",
    "\\begin{pmatrix} \n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "a_{i1} & a_{i2} & \\ldots & a_{in}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \n",
    "\\end{pmatrix} \n",
    "\\begin{pmatrix} \n",
    "\\vdots & b_{1j} & \\vdots \\\\ \n",
    "\\vdots & \\vdots & \\vdots \\\\ \n",
    "\\vdots & b_{nj} & \\vdots \n",
    "\\end{pmatrix} =  \n",
    "\\begin{pmatrix} \n",
    "\\vdots & \\vdots & \\vdots \\\\ \n",
    "\\vdots & a_{i1} b_{1j} + a_{i2} b_{2j} + \\ldots + a_{in} b_{nj} & \\vdots  \\\\ \n",
    "\\vdots & \\vdots & \\vdots \n",
    "\\end{pmatrix}.  \n",
    "$$  \n",
    "\n",
    "Next we give a few different ways of thinking about matrix multiplication.\n",
    "\n",
    "> **Proposition:** Let $A \\in \\mathcal{M}_{m \\times n}(\\mathbb{R}), B \\in \\mathcal{M}_{n \\times p}(\\mathbb{R})$. For any $j$ between $1$ and $p$, we think of the $j$-th column of $B$ as a vector in $\\mathbb{R}^n$ and denote it by the vector $\\boldsymbol{b}_j$. Then the $j$-th column of the matrix product $AB$ is the vector $A \\boldsymbol{b}_j$.  \n",
    "\n",
    "In other words,  \n",
    "$$  \n",
    "A \\begin{pmatrix} \\boldsymbol{b}_1 & \\mid & \\boldsymbol{b}_2 & \\mid & \\ldots & \\mid & \\boldsymbol{b}_p \\end{pmatrix} = \\begin{pmatrix} A \\boldsymbol{b}_1 & \\mid & A \\boldsymbol{b}_2 & \\mid & \\ldots & \\mid & A \\boldsymbol{b}_p \\end{pmatrix}.  \n",
    "$$  \n",
    "\n",
    "**Remark:** Since $A \\boldsymbol{b}_j$ is a matrix-vector product, this vector is formed by the entries of the vector $\\boldsymbol{b}_j$ specifying how to combine the columns of $A$.  \n",
    "\n",
    "**Remark:** This means that if we focus on the columns of the matrix $AB$, we can think of each column as a linear combination of the columns of $A$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we discuss the \"row picture\" of matrix multiplication.\n",
    "\n",
    "> **Proposition:**  \n",
    "> Let $A = (a_1 \\; \\ldots \\; a_n)$ be an $1 \\times n$ matrix (we call these *row vectors*) and let $B$ be an $n \\times p$ matrix. Denote the rows of $B$ with the row vectors $\\boldsymbol{b}_1^T, \\ldots , \\boldsymbol{b}_n^T$. Then the matrix product $AB$ is a row vector where  \n",
    "> $$  \n",
    "> AB = a_1 \\boldsymbol{b}_1^T + \\ldots + a_n \\boldsymbol{b}_n^T.  \n",
    "> $$  \n",
    "> In other words, the entries of the row vector $A$ specify how to combine the rows of $B$ to form the matrix product $AB$.  \n",
    "\n",
    "We can visualize this as follows:  \n",
    "$$  \n",
    "\\begin{pmatrix}  \n",
    "a_1 & \\ldots & a_n  \n",
    "\\end{pmatrix} \\begin{pmatrix}  \n",
    "\\boldsymbol{b}_1^T \\\\  \n",
    "\\hline \\vdots \\\\  \n",
    "\\hline  \n",
    "\\boldsymbol{b}_n^T  \n",
    "\\end{pmatrix} = a_1 \\boldsymbol{b}_1^T + \\ldots + a_n \\boldsymbol{b}_n^T.  \n",
    "$$  \n",
    "\n",
    "> **Proposition:**  \n",
    "> Let $A \\in \\mathcal{M}_{m \\times n}(\\mathbb{R}), B \\in \\mathcal{M}_{n \\times p}(\\mathbb{R})$. For any $i$ between 1 and $m$, we think of the $i$-th row of $A$ as a row vector and denote it by $\\boldsymbol{a}_i^T$. Then the $i$-th row of the matrix product $AB$ is the row vector $\\boldsymbol{a}_i^T B$.  \n",
    "\n",
    "In other words  \n",
    "$$  \n",
    "\\begin{pmatrix}  \n",
    "\\boldsymbol{a}_1^T \\\\  \n",
    "\\hline \\vdots \\\\  \n",
    "\\hline  \n",
    "\\boldsymbol{a}_n^T  \n",
    "\\end{pmatrix} B = \\begin{pmatrix}  \n",
    "\\boldsymbol{a}_1^T B \\\\  \n",
    "\\hline \\vdots \\\\  \n",
    "\\hline  \n",
    "\\boldsymbol{a}_n^T B  \n",
    "\\end{pmatrix}.  \n",
    "$$  \n",
    "\n",
    "\n",
    "**Remark:** Since $\\boldsymbol{a}_i^T B$ is a matrix product between a row vector and a matrix, this row vector is formed by the entries of the row vector $\\boldsymbol{a}_i^T$ specifying how to combine the rows of $B$. Therefore the rows of the matrix product $AB$ are linear combinations of the rows of $B$.  \n",
    "\n",
    "In summary, there are three equivalent ways of thinking about matrix multiplication.  \n",
    "\n",
    "1. Entry wise: the element in the $i$-th row and the $j$-th column of $AB$ is formed by using elements in the $i$-row of $A$ and the $j$-th column of $B$.  \n",
    "2. Column wise: the $j$-th column of $AB$ is formed by using elements of the $j$-th column of $B$ specifying how to combine the columns of $A$.  \n",
    "3. Row wise: the $i$-th row of $AB$ is formed by using elements of the $i$-th row of $A$ specifying how to combine the rows of $B$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Matrix Exponential\n",
    "\n",
    "Now we are ready to define the matrix exponential.\n",
    "\n",
    "First we recall the the power series representation of the exponential function $e^x$, \n",
    "$$\n",
    "e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots = \\sum_{j=0}^\\infty \\frac{x^j}{j!}, \\; x \\in \\mathbb{R}\n",
    "$$\n",
    "The radius of convergence of this power series is infinite. We will use this to motivate the defintion of the matrix exponential.\n",
    "\n",
    "> **Definition:** Let $A \\in \\mathcal{M}_{n \\times n}(\\mathbb{R})$. The *matrix exponential* of $A$, denoted by $e^A$, is defined via the power series\n",
    "> $$\n",
    "> \\mathcal{M}_{n \\times n}(\\mathbb{R}) \\ni e^{A} = I + A + \\frac{1}{2!}A^2 + \\frac{1}{3!} A^3 + \\cdots = \\sum_{j=0}^\\infty \\frac{1}{j!} A^j.\n",
    "> $$\n",
    "\n",
    "**Remark:** Since the matrix exponential is defined in terms of an infinite series, one in principle needs to ask if the series converges and in what sense. This is beyond the scope of this course, but with some tools one can show that this series converges for any matrix $A$.\n",
    "\n",
    "With the notion of the matrix exponential, we can define the matrix-valued function $\\Phi: \\mathbb{R} \\to \\mathcal{M}_{n \\times n}(\\mathbb{R})$ via $\\Phi(t) = e^{t A}$ for a given matrix $A$. \n",
    "\n",
    "The next proposition records some fundamental properties of the matrix-valued function $\\Phi$.\n",
    "\n",
    "> **Proposition:** Let $A \\in \\mathcal{M}_{n \\times n}(\\mathbb{R})$ and let the function $\\Phi: \\mathbb{R} \\to \\mathcal{M}_{n \\times n}(\\mathbb{R})$ be defined as above. The following hold. \n",
    "> 1. The derivative of $\\Phi$ is $A e^{tA}$ or $e^{tA} A$, i.e. $\\frac{d}{dt} e^{tA} = e^{tA} A = A e^{tA}$ (this is the product of two matrices).  \n",
    "> 2. $\\Phi$ equal to the identity matrix for $t = 0$, i.e. $e^{0 \\cdot A} = I$, the identity matrix.  \n",
    "> 3. $e^{(t+s) A} = e^{tA} e^{sA}$ for all $t,s \\in \\mathbb{R}$.  \n",
    "> 4. The inverse of $e^{tA}$ is the matrix $e^{-tA}$: $( e^{tA} )^{-1} = e^{-tA}$.  \n",
    "> 5. If $A = P D P^{-1}$ where $\\det P \\neq 0$ and $D$ is a diagonal matrix, then $e^{A} = P e^{D} P^{-1}$.  \n",
    "> 6. **If $AB = BA$**, then $e^{A+B} = e^A e^B$. Note in general we do not have equality since matrices in general do not commute.  \n",
    "\n",
    "The matrix exponential is important because of the following proposition.\n",
    "\n",
    "> **Proposition:**  The unique solution satisfying the IVP  \n",
    "> $$\n",
    "> \\begin{cases}\n",
    "> \\boldsymbol{X}'(t) = A \\boldsymbol{X}(t), \\; t \\in \\mathbb{R} \\\\\n",
    "> \\boldsymbol{X}(t_0) = \\boldsymbol{X}_0\n",
    "> \\end{cases}\n",
    "> $$\n",
    "> is given by the function $X : \\mathbb{R} \\to \\mathbb{R}^n$ defined via\n",
    "> $$\n",
    "> \\boldsymbol{X}(t) = e^{(t-t_0)A} \\boldsymbol{X}_0.\n",
    "> $$\n",
    "\n",
    "The point of the proposition above is to show that one can solve the IVP if one can compute $e^{tA}$ for a given matrix $A$. Unfortunately, in general computing $e^{tA}$ explicitly is quite cumbersome as it involves using *Jordan canonical form* of $A$ and the notion of generalized eigenvectors. This is outside the scope of this course, so we'll only consider a few special cases.\n",
    "\n",
    "If $A$ has $n$ real distinct eigenvalues and $n$ corresponding real linearly independent eigenvectors, we can make the following claim.\n",
    "\n",
    "> **Proposition:**  If $A$ has real distinct eigenvalues $\\lambda_1, ..., \\lambda_n$ and $n$ corresponding real linearly independent eigenvectors $\\boldsymbol{v}_1, ..., \\boldsymbol{v}_n$, then we can **diagonalize** $A = P D P^{-1}$ and  \n",
    "> $$\n",
    "> e^{tA} = P e^{tD} P^{-1}  \n",
    "> $$\n",
    "> where $P = \\begin{pmatrix} \\boldsymbol{v}_1 \\rvert & ...& \\lvert \\boldsymbol{v}_n \\end{pmatrix}$ and  \n",
    "> $$\n",
    "> e^{tD} = \\begin{pmatrix}\n",
    "> e^{t \\lambda_1} & 0 & \\cdots & 0\\\\\n",
    "> 0 & e^{t \\lambda_2} & \\cdots & 0 \\\\\n",
    "> 0 & \\vdots  & \\vdots & \\vdots \\\\\n",
    "> 0 & \\cdots & \\cdots & e^{t \\lambda_n}\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "\n",
    "An equivalent (and perhaps more familiar) way of writing the solution is to write  \n",
    "$$\n",
    "\\boldsymbol{y}(t) = c_1 e^{t\\lambda_1} \\boldsymbol{v}_1  + c_2 e^{t \\lambda_2} \\boldsymbol{v}_2 + ... + c_n e^{t \\lambda_n} \\boldsymbol{v}_n \n",
    "$$\n",
    "where $c_1, ..., c_n$ are constants determined by the initial condition. \n",
    "\n",
    "#### The inhomogeneous problem\n",
    "For the inhomogeneous problem  \n",
    "$$\n",
    "\\begin{cases}\n",
    "\\boldsymbol{X}'(t) = A \\boldsymbol{X}(t) + \\boldsymbol{F}(t), \\; t \\in \\mathbb{R} \\\\\n",
    "\\boldsymbol{X}(t_0) = \\boldsymbol{X}_0,\n",
    "\\end{cases}\n",
    "$$\n",
    "one can show that the unique solution is given by  \n",
    "$$\n",
    "\\boldsymbol{X}(t) = \\underbrace{e^{(t-t_0)A} \\boldsymbol{X}_0}_{= \\boldsymbol{X}_c} + \\underbrace{ e^{tA} \\int_{t_0}^t e^{-s A} \\boldsymbol{F}(s) \\; ds }_{ = \\boldsymbol{X}_p}. \n",
    "$$\n",
    "This is sometimes referred to as the variation of parameters formula or Duhamel's formula. One can readily check (using the product rule, fundamental theorem of calculus and Proposition) that  \n",
    "$$\n",
    "\\boldsymbol{X}'(t) = A e^{(t-t_0) A} \\boldsymbol{X}_0 + A e^{t A} \\int_{t_0}^t e^{-s A} \\boldsymbol{F}(s) \\; ds + e^{t A} e^{-tA} \\boldsymbol{F}(t)\n",
    "$$\n",
    "$$\n",
    "= A \\left( e^{(t-t_0) A} \\boldsymbol{X}_0 + e^{t A} \\int_{t_0}^t e^{-s A} \\boldsymbol{F}(s) \\; ds \\right) + e^{(t-t) A} \\boldsymbol{F}(t)  \n",
    "$$\n",
    "$$\n",
    "= A \\boldsymbol{X} + \\boldsymbol{F}(t), \\; t \\in \\mathbb{R}\n",
    "$$\n",
    "and  \n",
    "$$\n",
    "\\boldsymbol{X}(t_0) = e^{ (t_0 - t_0) A}  \\boldsymbol{X}_0  + e^{tA} \\int_{t_0}^{t_0} e^{-s A} \\boldsymbol{F}(s) \\; ds = I \\boldsymbol{X}_0 + e^{tA} \\boldsymbol{0} = \\boldsymbol{X}_0.\n",
    "$$\n",
    "Therefore, as long as one can calculate the matrix exponential $e^{tA}$ explicitly, one can write down the unique solution for the inhomogeneous problem. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example:**  Consider the inhomogeneous problem  \n",
    "> $$  \n",
    "> \\begin{cases}  \n",
    "> \\boldsymbol{X}'(t) = A \\boldsymbol{X}(t) + \\boldsymbol{F}(t), \\; t \\in \\mathbb{R} \\\\  \n",
    "> \\boldsymbol{X}(0) = \\boldsymbol{X}_0,  \n",
    "> \\end{cases}  \n",
    "> $$  \n",
    "> for  \n",
    "> $$  \n",
    "> A = \\begin{pmatrix}  \n",
    "> 0 & 1 \\\\  \n",
    "> -1 & 0  \n",
    "> \\end{pmatrix},\\quad  \\boldsymbol{F} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\boldsymbol{X}_0 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}.  \n",
    "> $$  \n",
    "> According to the discussion from the previous section, the unique solution to the system is given by  \n",
    "> $$  \n",
    "> \\boldsymbol{X}(t) = e^{tA} \\boldsymbol{X}_0 + e^{tA} \\int_0^t e^{-sA} \\boldsymbol{F}(s) \\; ds.  \n",
    "> $$  \n",
    "> From the previous example, we've found that  \n",
    "> $$  \n",
    "> e^{tA} = \\begin{pmatrix}  \n",
    "> \\cos t  & \\sin t \\\\  \n",
    "> -\\sin t & \\cos t  \n",
    "> \\end{pmatrix},  \n",
    "> $$  \n",
    "> therefore the solution to the system is $X: \\mathbb{R} \\to \\mathbb{R}^2$ given by\n",
    "> $$  \n",
    "> \\begin{align*}  \n",
    "> \\boldsymbol{X}(t) &= e^{tA} \\boldsymbol{X}_0 + e^{tA} \\int_0^t e^{-sA} \\boldsymbol{F}(s) \\; ds\\\\  \n",
    "> &= \\begin{pmatrix}  \n",
    "> \\cos t  & \\sin t \\\\  \n",
    "> -\\sin t & \\cos t  \n",
    "> \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}  + \\begin{pmatrix}  \n",
    "> \\cos t  & \\sin t \\\\  \n",
    "> -\\sin t & \\cos t  \n",
    "> \\end{pmatrix} \\int_0^t \\begin{pmatrix}  \n",
    "> \\cos (-s)  & \\sin (-s) \\\\  \n",
    "> -\\sin (-s) & \\cos (-s)  \n",
    "> \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\; ds \\\\  \n",
    "> &= \\begin{pmatrix}  \n",
    "> \\cos t  & \\sin t \\\\  \n",
    "> -\\sin t & \\cos t  \n",
    "> \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}  + \\begin{pmatrix}  \n",
    "> \\cos t  & \\sin t \\\\  \n",
    "> -\\sin t & \\cos t  \n",
    "> \\end{pmatrix} \\int_0^t \\begin{pmatrix}  \n",
    "> \\cos s  & -\\sin s \\\\  \n",
    "> \\sin s & \\cos s  \n",
    "> \\end{pmatrix}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\; ds \\\\  \n",
    "> &=\\begin{pmatrix} \\sin t \\\\ \\cos t \\end{pmatrix} + \\begin{pmatrix}  \n",
    "> \\cos t  & \\sin t \\\\  \n",
    "> -\\sin t & \\cos t  \n",
    "> \\end{pmatrix} \\int_0^t \\begin{pmatrix} \\cos s \\\\ \\sin s \\end{pmatrix} \\; ds \\\\  \n",
    "> &=\\begin{pmatrix} \\sin t \\\\ \\cos t \\end{pmatrix} + \\begin{pmatrix}  \n",
    "> \\cos t  & \\sin t \\\\  \n",
    "> -\\sin t & \\cos t  \n",
    "> \\end{pmatrix} \\begin{pmatrix}  \n",
    "> \\sin t \\\\  \n",
    "> -\\cos (t) + 1  \n",
    "> \\end{pmatrix} \\\\  \n",
    "> &= \\begin{pmatrix} \\sin t \\\\ \\cos t \\end{pmatrix} + \\begin{pmatrix}  \n",
    "> \\cos t  \\sin t - \\sin t \\cos t + \\sin t  \\\\  \n",
    "> -\\sin^2 t - \\cos^2 t + \\cos t  \n",
    "> \\end{pmatrix} \\\\  \n",
    "> &= \\begin{pmatrix}  \n",
    "> 2\\sin t \\\\  \n",
    "> 2\\cos t - 1  \n",
    "> \\end{pmatrix}.  \n",
    "> \\end{align*}\n",
    "> $$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
