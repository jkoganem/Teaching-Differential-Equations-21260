{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Higher Order Linear Equations**\n",
    "\n",
    "---\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "This notebook introduces concepts and techniques useful for studying higher order equations. \n",
    "\n",
    "---\n",
    "\n",
    "### **Author**\n",
    "**Junichi Koganemaru**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Last Updated**\n",
    "**January 28, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we aim to study general $n$-th order linear differential equations of the form\n",
    "\n",
    "$$\n",
    "a_n(t) y^{(n)}(t) + a_{n-1}(t) y^{(n-1)}(t) + \\cdots + a_1(t) y'(t) + a_0(t) y(t) = f(t), \\; t \\in \\mathbb{R}\n",
    "$$\n",
    "where $a_0, ..., a_n, f: \\mathbb{R} \\to \\mathbb{R}$ are assumed to be continuous. \n",
    "\n",
    "The focus for us will be the case when $n = 2$, as many physical systems are modeled via second order differential equations. However, we will see that the theory for higher order equations is completely analogous.\n",
    "\n",
    "First, we go over an example to illustrate why the concepts we introduce in the next section are useful.\n",
    "\n",
    "> **Example:**\n",
    "> Consider the second order linear differential equation\n",
    "> $$\n",
    "> y''(t) + 2y'(t) + 2y(t) = f(t), \\; t \\in \\mathbb{R},\n",
    "> $$\n",
    "> where $f$ is assumed to be continuous. If $y: \\mathbb{R} \\to \\mathbb{R}$ is a solution to the equation above, consider the **vector-valued** function $\\boldsymbol{Y}: \\mathbb{R} \\to \\mathbb{R}^2$ defined via\n",
    "> $$\n",
    "> \\boldsymbol{Y}(t) = \\begin{pmatrix} y(t) \\\\ y'(t) \\end{pmatrix}.\n",
    "> $$\n",
    "> Here $\\boldsymbol{Y}$ takes in a value $t \\in \\mathbb{R}$ and gives back a **column vector** in $\\mathbb{R}^2$. We will carefully introduce operations that can be performed on vector-valued functions later, for now we just need to know how to differentiate them. For vector valued functions, their derivatives are defined **component wise**, i.e. \n",
    "> $$\n",
    "> \\boldsymbol{Y}'(t) = \\begin{pmatrix} \\frac{d}{dt} [y(t)] \\\\ \\frac{d}{dt} [y'(t)] \\end{pmatrix} = \\begin{pmatrix} y'(t) \\\\ y''(t) \\end{pmatrix}, \\; t \\in \\mathbb{R}.\n",
    "> $$\n",
    "> Note that since $y$ is a solution to the differential equation, we have that $y''(t) = -2y'(t) - 2y(t) + f(t)$. Therefore, we have \n",
    "> $$\n",
    "> \\boldsymbol{Y}'(t) = \\begin{pmatrix} y'(t) \\\\ y''(t) \\end{pmatrix} = \\begin{pmatrix} y'(t) \\\\ -2y'(t) - 2y(t) + f(t) \\end{pmatrix} =  \\begin{pmatrix} 0 \\cdot y(t) + 1 \\cdot y'(t) \\\\ - 2 \\cdot y(t) -2 \\cdot y'(t)  \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ f(t) \\end{pmatrix}.\n",
    "> $$\n",
    "> Note that the first term on the right hand side only depends on the entries of $\\boldsymbol{Y}$. Using some notation that we will go over later, we can write the above equation as\n",
    "> $$\n",
    "> \\boldsymbol{Y}'(t) =  \\begin{pmatrix}  0 & 1 \\\\ -2 & -2 \\end{pmatrix} \\begin{pmatrix} y(t) \\\\ y'(t) \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ f(t) \\end{pmatrix},\n",
    "> $$\n",
    "> where \n",
    "> $$ \n",
    "> A = \\begin{pmatrix}  0 & 1 \\\\ -2 & -2 \\end{pmatrix}\n",
    "> $$\n",
    "> is a **matrix** recording the coefficients of the components of $\\boldsymbol{Y}$ on the right hand side of the equation. We thus obtain a **first order vector-valued** equation  \n",
    "> $$\n",
    "> \\boldsymbol{Y}'(t) = A \\boldsymbol{Y}(t) + \\boldsymbol{F}(t) \\Longleftrightarrow \\boldsymbol{Y}'(t) - A  \\boldsymbol{Y}(t) =\\boldsymbol{F}(t), \\; t \\in \\mathbb{R},\n",
    "> $$\n",
    "> where $F: \\mathbb{R} \\to \\mathbb{R}^2$ is defined via $\\boldsymbol{F}(t) = \\begin{pmatrix} 0 \\\\ f(t) \\end{pmatrix}$. This is a **first order linear differential equation** for the vector-valued function $\\boldsymbol{Y}$.\n",
    "\n",
    "The upshot of this example is that we can rewrite a second order scalar differential equation as a first order vector-valued differential equation. \n",
    "\n",
    "In fact, this can be done for general $n$-th order linear equations as well, which means that we can always trade the order of the differential equation for the number of components of the vector-valued function $\\boldsymbol{Y}$. \n",
    "\n",
    "While this is extremely powerful, as it shows that any $n$-th order scalar equation can be studied as a first order vector-valued equation, it requires one to be comfortable with the language of linear algebra. We will take this approach later as it gives us a unified framework for studying linear equations.\n",
    "\n",
    "For now, we will only study second order equations as scalar equations, however the example shows that the theory of $n$-th order linear equations must be intricately connected to concepts coming from linear algebra. As such, we need to introduce some preliminary notions below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries \n",
    "\n",
    "### Linear Independence\n",
    "Linear independence is a notion from linear algebra that encodes the \"dependencies\" among a collection of objects.\n",
    "\n",
    "It is then useful in this discussion to borrow some terminology from linear algebra.\n",
    "\n",
    "> **Definition (Linear combination of functions)**\n",
    "> Given $n$ continuous functions $y_1, \\ldots, y_n: I \\to \\mathbb{R}$ on an interval $I$, a linear combination of these functions is another continuous function $y: I \\to \\mathbb{R}$ which can be written as \n",
    "> $$\n",
    "> y(t) = c_1 y_1(t) + c_2 y_2(t) + \\ldots + c_n y_n(t), \\; t \\in I\n",
    "> $$\n",
    "> where $c_1, \\ldots, c_n \\in \\mathbb{R}$ are constants.\n",
    "\n",
    "To motivate our discussion, let's first consider an example.\n",
    "\n",
    "> **Example**\n",
    "> Consider the functions $y_1, y_2, y_3: \\mathbb{R} \\to \\mathbb{R}$ defined via $y_1(t) = t, y_2(t) = t+1, y_3(t) = 2t+1$ for $t \\in \\mathbb{R}$, and suppose that $y$ is a linear combination of $y_1, y_2, y_3$: \n",
    "> $$\n",
    "> y(t) = c_1 y_1(t) + c_2 y_2(t) + c_3 y_3(t), \\; t \\in \\mathbb{R} \n",
    "> $$\n",
    "> for some constants $c_1, c_2, c_3$. Notice that here we have \n",
    "> $$\n",
    "> y_1 + y_2 = y_3,\n",
    "> $$\n",
    "> so in the equation above, we can replace $y_3$ and write \n",
    "> $$\n",
    "> y(t) = c_1 y_1(t) + c_2 y_2(t) + c_3(y_1(t) + y_2(t)) = C_1 y_1(t) + C_2 y_2(t), \\; t \\in \\mathbb{R}\n",
    "> $$\n",
    "> where $C_1 = c_1 + c_3, C_2 = c_2 + c_3$.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  What this shows us is that because $y_3$ is \"dependent\" on $y_1$ and $y_2$, even though in the definition of $y$ there seems to be three \"building blocks\", in reality only two \"building blocks\" are required to create $y$. In other words, any linear combination of $y_1, y_2, y_3$ can always be written as a linear combination of $y_1, y_2$.\n",
    "  \n",
    "  In this sense, the appearance of $y_3$ is redundant.\n",
    "\n",
    "We will see later that general solutions to homogeneous linear differential equation is given as a linear combination of functions. What we want to do is find a way to capture the notion of \"non-redundancy\". This is where the notion of **linear independence** comes in. First, let us give an intuitive definition as to what it means for a set of functions to be linearly independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition (Linear independence I)**\n",
    "> A set of continuous functions defined over an interval $I$ is said to be *linearly independent* if none of the functions can be written as the linear combination of the other functions in the set, on the interval $I$.\n",
    "\n",
    "In the previous example, the set $\\{y_1, y_2, y_3\\}$ is not linearly independent over $\\mathbb{R}$ because $y_3$ is a linear combination of $y_1$ and $y_2$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous definition, while conceptually clear, is hard to apply in practice. Instead we will often use the following alternative definition.\n",
    "\n",
    "> **Definition (Linear independence II)**\n",
    "> A set of continuous functions $y_1, \\ldots, y_n: I \\to \\mathbb{R}$ is said to be linearly independent on an interval $I$ if \n",
    "> $$\n",
    "> c_1 y_1(t) + \\ldots + c_n y_n(t) = 0 \\; \\text{for all} \\; t \\in I\n",
    "> $$\n",
    "> implies $c_1 = c_2 = \\ldots = c_n = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see why the two definitions are equivalent to each other. Suppose there exists some non-zero $c_i$'s such that \n",
    "$$\n",
    "c_1 y_1(t) + \\ldots + c_k y_k(t)= 0, \\; t \\in I.\n",
    "$$\n",
    "Up to relabeling of the indices, we can suppose without loss of generality that $c_1 \\neq 0$. Then we can immediately write \n",
    "$$\n",
    "y_1(t) = -\\frac{1}{c_1} \\left( c_2 y_2(t) + \\ldots + c_k y_k(t) \\right), \\; t \\in I\n",
    "$$\n",
    "meaning that $y_1$ is a linear combination of other functions in the set. On the other hand, if a function is already a linear combination of the other functions, then we can find non-zero coefficients to produce the zero function. So the two definitions are equivalent to each other.\n",
    "\n",
    "**Remark:** According to this definition, any set containing the zero function is automatically linearly dependent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example**\n",
    "> Consider the functions $y_1, y_2, y_3$ defined via $y_1(t) = e^t, y_2(t) = e^{-t}, y_3(t) = e^{t} + e^{-t}$ for $t \\in \\mathbb{R}$. Notice that\n",
    "> $$\n",
    "> y_3(t) = y_1(t) + y_2(t) \\Longleftrightarrow y_1(t) + y_2(t) - y_3(t) = 0, \\; t \\in \\mathbb{R}\n",
    "> $$\n",
    "> for all real values of $t$. So if $c_1 y_1(t) + c_2 y_2(t) + c_3 y_3(t) = 0$ for all $t \\in \\mathbb{R}$, it's not necessarily true that $c_1 = c_2 = c_3 = 0$. Therefore these three functions are linearly dependent.\n",
    "\n",
    "> **Example**\n",
    "> Consider the functions $y_1, y_2, y_3$ defined via $y_1(t) = \\sin^2(t), y_2(t) = \\cos^2(t), y_3(t) = 1$ for $t \\in \\mathbb{R}$. Since \n",
    "> $$\n",
    "> y_1(t) + y_2(t) = \\sin^2(t) + \\cos^2(t) = 1 = y_3(t)\n",
    "> $$\n",
    "> for all $t \\in \\mathbb{R}$, we see that these functions are linearly dependent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wronskian\n",
    "\n",
    " In practice, it is usually cumbersome to check linear independence via either of the previous definition when the set contains more than two functions. So instead, we'll again borrow some ideas from linear algebra and encode the dependence/independence of these functions in a single object - the Wronskian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition**\n",
    "> Let $I \\subseteq \\mathbb{R}$ be an interval and let $y_1, ..., y_n \\in C^{n-1}(I ; \\mathbb{R})$. Then the *Wronskian* of this collection of functions is a function $W: I \\to \\mathbb{R}$ defined as a **determinant** of a matrix,\n",
    "> $$\n",
    "> W(y_1, y_2, ..., y_n) (t) := \\det \\begin{pmatrix}\n",
    "> y_1(t) & y_2(t) & \\cdots & y_n(t) \\\\\n",
    "> y_1'(t) & y_2'(t) & \\cdots & y_n'(t) \\\\\n",
    "> \\vdots & \\vdots & \\ddots &\\vdots  \\\\\n",
    "> y_1^{(n-1)}(t) & y_2^{(n-1)}(t) & \\cdots & y_n^{(n-1)}(t) \n",
    "> \\end{pmatrix}, \\; t \\in I.\n",
    "> $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following proposition establishes a connection between the Wronskian and linear independence.\n",
    "\n",
    "> **Proposition**\n",
    "> Let $y_1, \\ldots , y_n: I \\to \\mathbb{R}$ be $n$ **analytic functions** over an interval $I$. Then the set of functions are linearly dependent if and only if $W(y_1,\\ldots ,y_n)$ is identically zero on $I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words,\n",
    "1. If the Wronskian of $n$ analytic functions is identically zero over an interval $I$, then the set of functions is linearly dependent.\n",
    "2. Otherwise, the set of functions is linearly independent.\n",
    "\n",
    "This means as long as we can compute the Wronksian of a set of analytic functions, we can determine whether they are linearly independent or not.\n",
    "\n",
    "**Remark:** Analytic functions are smooth functions that admit a local power series representation at every point in their respective domains. In practice, many functions that we encounter are analytic, so this theorem is quite useful. Constant functions, trigonometric functions, polynomials, and exponential functions are all examples of analytic functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the functions in question are solutions to linear differential equation, we can say something even stronger. \n",
    "\n",
    "> **Theorem (Cauchy–Kovalevskaya)** \n",
    "> Let $y_1, ..., y_n \\in C^{n-1}(\\mathbb{R})$ be $n$ solutions to an $n$-th order homogeneous linear differential equation on an interval $I$ of the form \n",
    "> $$\n",
    "> y^{(n)}(t) + a_{n-1}(t) y^{(n-1)}(t) + \\ldots  + a_1(t)y'(t) + a_0(t) y(t) = 0, \\; t \\in I\n",
    "> $$\n",
    "> where $\\{a_i\\}_{i=1}^{n-1}$ are *analytic* over $I$. Then the following hold. \n",
    "> 1. $y_1, ..., y_n$ are analytic \n",
    "> 2. Either $W(y_1, y_2, ..., y_n)(t) = 0$ for all $t \\in I$ and the set $\\{y_1, ..., y_n\\}$ is linearly dependent, or $W(y_1, y_2, ..., y_n)(t) \\neq 0$ for all $t \\in I$ and the set $\\{y_1, ..., y_n\\}$ is linearly independent.\n",
    "\n",
    "**Remark**: This theorem is stated only for functions that are solutions to homogeneous equations of a specific form (leading coefficient must be 1 and the coefficients must be analytic). By item 2, as long as the Wronskian does not vanish at one point, we can immediately conclude that the set of functions is linearly independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinants \n",
    "Next we discuss how to compute the Wronskian by going over how to compute determinants.\n",
    "\n",
    "### Determinants of $2 \\times 2$ matrices\n",
    "\n",
    "We first consider the determinant of $2 \\times 2$ matrices.\n",
    "\n",
    "> **Definition**  \n",
    "> Consider the square matrix  \n",
    "> $$  \n",
    "> A = \\begin{pmatrix}  \n",
    "> a & b \\\\  \n",
    "> c & d  \n",
    "> \\end{pmatrix}.  \n",
    "> $$  \n",
    "> The *determinant* of $A$ is a number associated to the matrix $A$, defined by  \n",
    "> $$  \n",
    "> \\det A := ad - bc.  \n",
    "> $$  \n",
    "\n",
    "**Remark** \n",
    "The determinant is sometimes denoted with vertical bars: \n",
    "$$\n",
    "\\det(A) = \\begin{vmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{vmatrix}.\n",
    "$$\n",
    "\n",
    "> **Example:**\n",
    "> Let \n",
    "> $$\n",
    "> A = \\begin{pmatrix} \n",
    "> 1 & 2 \\\\\n",
    "> 3 & 4 \n",
    "> \\end{pmatrix}, B = \\begin{pmatrix} \n",
    "> 2 & 3 \\\\\n",
    "> 4 & 5\n",
    "> \\end{pmatrix}. \n",
    "> $$\n",
    "> Then\n",
    "> $$\n",
    "> \\det A = 1 \\times 4 - 2 \\times 3 = -2, \\det B = 2 \\times 5 - 3 \\times 4 = -2.\n",
    "> $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Determinants of $3 \\times 3$ matrices\n",
    "\n",
    "Suppose $A$ is a $3 \\times 3$ matrix of the form\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "The simplest way to calculate $\\det(A)$ is to use\n",
    "$$\n",
    "\\det(A) = \\begin{vmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{vmatrix} = a  \\begin{vmatrix}\n",
    "e & f \\\\\n",
    "h & i\n",
    "\\end{vmatrix} - d \\begin{vmatrix}\n",
    "b & c \\\\\n",
    "h & i\n",
    "\\end{vmatrix}  + g\\begin{vmatrix}\n",
    "b & c \\\\\n",
    "e & f\n",
    "\\end{vmatrix}\n",
    "= a(ei - fh) - d(bi - ch) + g( bf - ce).\n",
    "$$\n",
    "\n",
    "One way to interpret this is that we are expanding along the first column, and we're multiplying each element in the column by the determinant of the matrix obtained from ignoring the row and column containing that element. Here's the way to visualize it (the smaller matrices are called \\emph{minors})\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "a & \\cdots & \\cdots \\\\\n",
    "\\vdots & \\textcolor{red}{e} & \\textcolor{red}{f}\\\\\n",
    "\\vdots & \\textcolor{red}{h} & \\textcolor{red}{i}\n",
    "\\end{vmatrix}  \\quad \\quad \\begin{vmatrix}\n",
    "\\vdots & \\textcolor{red}{b} & \\textcolor{red}{c}\\\\\n",
    "d & \\cdots & \\cdots\\\\\n",
    "\\vdots & \\textcolor{red}{h} & \\textcolor{red}{i}\n",
    "\\end{vmatrix} \\quad \\quad \n",
    "\\begin{vmatrix}\n",
    "\\vdots & \\textcolor{red}{b} & \\textcolor{red}{c}\\\\\n",
    "\\vdots & \\textcolor{red}{e} & \\textcolor{red}{f}\\\\\n",
    "g & \\cdots & \\cdots\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "Notice that the signs in the expansion flipped from $+a$ to $-d$ to $+g$. This is important. \n",
    "\n",
    "In general one can expand along any row and any column, as long as you have the right sign in front of the elements in the expansion. The signs associated with each element is given in the matrix on the right:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{pmatrix} \\quad \\quad \\begin{pmatrix}\n",
    "+ & - & +\\\\\n",
    "- & +& -\\\\\n",
    "+ & - & +\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So in the equation above\n",
    "$$\n",
    "\\det(A) = \\textcolor{red}{+a}  \\begin{vmatrix}\n",
    "e & f \\\\\n",
    "h & i\n",
    "\\end{vmatrix} \\textcolor{red}{-d} \\begin{vmatrix}\n",
    "b & c \\\\\n",
    "h & i\n",
    "\\end{vmatrix}  \\textcolor{red}{+g} \\begin{vmatrix}\n",
    "b & c \\\\\n",
    "e & f\n",
    "\\end{vmatrix},\n",
    "$$\n",
    "we took\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\textcolor{red}{a} & b & c\\\\\n",
    "\\textcolor{red}{d} & e & f\\\\\n",
    "\\textcolor{red}{g} & h & i\n",
    "\\end{pmatrix} \\quad \\quad \\begin{pmatrix}\n",
    "\\textcolor{red}{+} & - & +\\\\\n",
    "\\textcolor{red}{-} & +& -\\\\\n",
    "\\textcolor{red}{+} & - & +\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "If we were to expand along the second column then we'd have the minors as \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\cdots & b & \\cdots \\\\\n",
    "\\textcolor{blue}{d} & \\vdots  & \\textcolor{blue}{f}\\\\\n",
    "\\textcolor{blue}{g} & \\vdots & \\textcolor{blue}{i}\n",
    "\\end{pmatrix}  \\quad \\quad \\begin{pmatrix}\n",
    "\\textcolor{blue}{a} & \\vdots &  \\textcolor{blue}{c} \\\\\n",
    "\\cdots & e  & \\cdots\\\\\n",
    "\\textcolor{blue}{g} & \\vdots & \\textcolor{blue}{i}\n",
    "\\end{pmatrix} \\quad \\quad \n",
    "\\begin{pmatrix}\n",
    "\\textcolor{blue}{a} & \\vdots &  \\textcolor{blue}{c} \\\\\n",
    "\t\\textcolor{blue}{g} & \\vdots & \\textcolor{blue}{i}\n",
    "   \\end{pmatrix}  \\quad \\quad \\begin{pmatrix}\n",
    "\t\\textcolor{blue}{a} & \\vdots &  \\textcolor{blue}{c} \\\\\n",
    "\t\\cdots & e  & \\cdots\\\\\n",
    "\t\\textcolor{blue}{g} & \\vdots & \\textcolor{blue}{i}\n",
    "   \\end{pmatrix} \\quad \\quad \n",
    "   \\begin{pmatrix}\n",
    "   \\textcolor{blue}{a} & \\vdots &  \\textcolor{blue}{c} \\\\\n",
    "\t\\textcolor{blue}{d} & \\vdots  & \\textcolor{blue}{f}\\\\\n",
    "   \\cdots& h & \\cdots\n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "   and thus \n",
    "   $$\n",
    "   \\det(A) =  \\textcolor{red}{-b}  \\begin{vmatrix}\n",
    "   d & f \\\\\n",
    "   g & i\n",
    "   \\end{vmatrix}  \\textcolor{red}{+e} \\begin{vmatrix}\n",
    "   a & c \\\\\n",
    "   g & i\n",
    "   \\end{vmatrix} \\textcolor{red}{-h} \\begin{vmatrix}\n",
    "   a & c \\\\\n",
    "   d & f\n",
    "   \\end{vmatrix}.\n",
    "   $$\n",
    "\n",
    "> **Example:**\n",
    "> Let \n",
    "> $$\n",
    "> A = \\begin{pmatrix} \n",
    "> 1 & 2 & 3 \\\\\n",
    "> 4 & 5 & 6 \\\\\n",
    "> 7 & 8 & 9\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> According to the discussion above, we can evaluate $\\det$ along the second row:\n",
    "> $$\n",
    "> \\det A = -4 \\times \\det \\begin{pmatrix} \n",
    "> 2 & 3 \\\\\n",
    "> 8 & 9\n",
    "> \\end{pmatrix} + 5 \\times \\det \\begin{pmatrix} \n",
    "> 1 & 3 \\\\\n",
    "> 7 & 9\n",
    "> \\end{pmatrix} - 6 \\times  \\det \\begin{pmatrix} \n",
    "> 1 & 2 \\\\\n",
    "> 7 & 8\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> We can also evaluate along the second column:\n",
    "> $$\n",
    "> \\det A = -2 \\times \\det \\begin{pmatrix} \n",
    "> 4 & 6 \\\\\n",
    "> 7 & 9\n",
    "> \\end{pmatrix} + 5 \\times \\det  \\begin{pmatrix} \n",
    "> 1 & 3 \\\\\n",
    "> 7 & 9\n",
    "> \\end{pmatrix} - 8 \\times \\det \\begin{pmatrix} \n",
    "> 1 & 3 \\\\\n",
    "> 4 & 6\n",
    "> \\end{pmatrix}.  \n",
    "> $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinants of $n \\times n$ matrices\n",
    "\n",
    "Now we consider determinants of $n \\times n$ matrices.\n",
    "\n",
    "> **Definition (Determinants via cofactor expansion along first row)**  \n",
    "> Let $A = (a_{ij})$ be an $n \\times n$ square matrix. Denote by $\\tilde{A}_{ij}$ the $(n-1) \\times (n-1)$ matrix obtained from deleting the row and column containing $a_{ij}$ from $A$. Then we define the determinant of $A$ to be \n",
    "> $$\n",
    "> \\det A := \\sum_{k=1}^n (-1)^{1 +k} a_{1k} \\det \\tilde{A}_{1k}.\n",
    "> $$\n",
    "\n",
    "One can show that, in fact, you can expand along any column or any row. \n",
    "\n",
    "> **Proposition (Cofactor expansion along any row)**  \n",
    "> Let $i$ be any integer between $1$ and $n$. Then the determinant of $A$ is equal to \n",
    "> $$\n",
    "> \\det A = \\sum_{k=1}^n (-1)^{i + k} a_{ik} \\det \\tilde{A}_{ik}.\n",
    "> $$\n",
    "\n",
    "> **Proposition (Cofactor expansion along any column)**  \n",
    "> Let $j$ be any integer between $1$ and $n$. Then the determinant of $A$ is equal to \n",
    "> $$\n",
    "> \\det A = \\sum_{k=1}^n (-1)^{j + k} a_{kj} \\det \\tilde{A}_{kj}.\n",
    "> $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples calculating Wronskians\n",
    "\n",
    "> **Example**\n",
    "> Consider the functions $y_1, y_2: \\mathbb{R} \\to \\mathbb{R}$ defined via \n",
    "> $$\n",
    "> y_1(x) = e^x, \\; y_2(x) = e^{2x}.\n",
    "> $$\n",
    "> Then \n",
    "> $$\n",
    "> W(y_1,y_2) (x) = \\det \\begin{pmatrix}\n",
    "> e^x & e^{2x} \\\\\n",
    "> e^x & 2e^{2x}\n",
    "> \\end{pmatrix} = 2e^{3x} - e^{3x} = e^{3x} \\neq 0 \\; \\text{for all} \\; x \\in \\mathbb{R}.\n",
    "> $$\n",
    "> Therefore the two functions are linearly independent over any interval $I \\subseteq \\mathbb{R}$.  \n",
    "\n",
    "> **Example**\n",
    "> Consider the functions $y_1, y_2: \\mathbb{R} \\to \\mathbb{R}$ defined via \n",
    "> $$\n",
    "> y_1(t) = t^2, \\; y_2(t) = t^3.\n",
    "> $$\n",
    "> Then \n",
    "> $$\n",
    "> W(y_1,y_2) (t) = \\det \\begin{pmatrix}\n",
    "> t^2 & t^3 \\\\\n",
    "> 2t & 3t^2\n",
    "> \\end{pmatrix} = 3t^4 - 2t^4 = t^4 \\neq 0 \\; \\text{for all} \\; t \\neq 0.\n",
    "> $$\n",
    "> Therefore the two functions are linearly independent on any interval $I \\subseteq \\mathbb{R}$, even if it includes $t = 0$.  \n",
    "\n",
    "**Remark:**\n",
    "Since the Wronskian in the previous example doesn't vanish identically on any interval $I$ containing $0$, this also means that on any interval $I$ containing 0, there is no equation of the form \n",
    "$$\n",
    "y''(t) + a_1(t) y'(t) + a_0(t)y(t) = 0, \\; t \\in I\n",
    "$$\n",
    "with analytic coefficients that has these two functions as solutions, as otherwise either the Wronskian is either never zero or it vanishes identically. \n",
    "\n",
    "However, if $I$ does not contain 0, then the Wronskian never vanishes and this is possible. One can check that $y_1, y_2: (0,\\infty) \\to \\mathbb{R}$ are solutions to \n",
    "$$\n",
    "y''(t) - \\frac{5}{2t} y'(t) + \\frac{3}{2t^2} y(t) = 0, \\; t > 0.\n",
    "$$\n",
    "on the interval $I = (0,\\infty)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example**\n",
    "> Consider the functions $y_1, y_2, y_3: \\mathbb{R} \\to \\mathbb{R}$ defined via \n",
    "> $$\n",
    "> y_1(t) = \\sin^2(t), \\; y_2(t) = \\cos^2(t), \\; y_3(t) = 1.\n",
    "> $$\n",
    "> Then \n",
    "> $$\n",
    "> W(y_1,y_2, y_3) (t) = \\det \\begin{pmatrix}\n",
    "> \\sin^2(t) & \\cos^2(t) & 1 \\\\\n",
    "> 2 \\sin(t) \\cos(t) & -2 \\sin(t)\\cos(t) & 0 \\\\\n",
    "> 2 \\cos(2t) & -2 \\cos(2t) & 0\n",
    "> \\end{pmatrix}\n",
    "> = \\det \\begin{pmatrix}\n",
    "> 2 \\sin(t) \\cos(t) & -2 \\sin(t)\\cos(t) \\\\\n",
    "> 2 \\cos(2t) & -2 \\cos(2t)\n",
    "> \\end{pmatrix}\n",
    "> = 0 \\; \\text{for all} \\; t \\in \\mathbb{R}.\n",
    "> $$\n",
    "> Therefore the three functions are linearly dependent on any interval $I \\subseteq \\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we state a version of the existence and uniqueness for higher order linear differential equations.\n",
    "\n",
    "> **Theorem (Existence and uniqueness theorem)**\n",
    "> Let $t_0 \\in \\mathbb{R}$ and let $I$ be an interval containing $t_0$. Consider an initial value of the form \n",
    "> $$\n",
    "> \\begin{cases}\n",
    "> a_n(t) y^{(n)}(t) + \\ldots + a_1(t) y'(t) + a_0(t)y(t)  = 0, & t \\in I \\\\\n",
    "> y^{(k)}(t_0) = y_k, \\; 0 \\le k \\le n-1.\n",
    "> \\end{cases} \n",
    "> $$\n",
    "> If $a_n, \\ldots, a_0: I \\to \\mathbb{R} $ are continuous and $a_n(t) \\neq 0$ for all $t \\in I$, then there exists a unique solution that is defined globally on $I$. \n",
    "\n",
    "As a consequence of the existence and uniqueness theorem, we can deduce the structure of general solutions to homogeneous linear equations. \n",
    "\n",
    "> **Proposition**\n",
    "> Let $I$ be an interval and let $a_n, \\ldots, a_0: I \\to \\mathbb{R}$ be continuous, and assume $a_n(t) \\neq 0$ for all $t \\in I$. Given an $n$-th order homogeneous linear ODE of the form \n",
    "> $$\n",
    "> L(y) (t) = a_n(t) y^{(n)}(t) + \\ldots + a_1(t) y'(t) + a_0(t)y(t)  = 0,\n",
    "> $$\n",
    "> the general solution to this equation is given by \n",
    "> $$\n",
    "> y_h(t) = c_1 y_1(t) + \\ldots + c_n y_n(t), \\; t \\in I\n",
    "> $$\n",
    "> where $y_1, \\ldots, y_n$ are linearly independent over $I$ and $c_1,\\ldots,c_n$ are arbitrary constants.\n",
    "\n",
    "**Justification:** For the sake of simplicity we will only show this for the case of $n=2$. Suppose we'd like to identify the unique solution to the IVP\n",
    "$$\n",
    "\\begin{cases}\n",
    "a_2(t) y''(t) + a_1(t) y'(t) + a_0(t)y(t) = 0, \\; t \\in I \\\\\n",
    "y(t_0) = c_1 \\\\\n",
    "y'(t_0) = c_2.\n",
    "\\end{cases}\n",
    "$$\n",
    "By uniqueness, there exists unique solutions $y_1, y_2$ defined over $I$ to the IVPs \n",
    "$$\n",
    "\\begin{cases}\n",
    "a_2(t) y''(t) + a_1(t) y'(t) + a_0(t)y(t) = 0, \\; t \\in I\\\\\n",
    "y(t_0) = 1 \\\\\n",
    "y'(t_0) = 0.\n",
    "\\end{cases}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\begin{cases}\n",
    "a_2(t) y''(t) + a_1(t) y'(t) + a_0(t)y(t) = 0, \\; t \\in I\\\\\n",
    "y(t_0) = 0 \\\\\n",
    "y'(t_0) = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "respectively, since the coefficients $a_2, a_1, a_0$ are assumed to be continuous and $a_2(t) \\neq 0$ for all $t \\in I$. Then if we define the function $y$ via $y(t) = c_1y_1(t) + c_2y_2(t), t \\in I$, by linearity,\n",
    "$$\n",
    "L(y) = c_1 L(y_1) + c_2 L(y_2) = c_1 (0) + c_2(0) = 0,\n",
    "$$\n",
    "and also \n",
    "\\begin{align}\n",
    "y(t_0) &= c_1 y_1(t_0) + c_2 y_2(t_0) = c_1 (1) + c_2 (0) = c_1 \\\\\n",
    "y'(t_0) &= c_1 y_1'(t_0) + c_2 y_2'(t_0) = c_1 (0) + c_2 (1) = c_2.\n",
    "\\end{align}\n",
    "So by uniqueness, $y = c_1 y_1(t) + c_2 y_2(t)$ is the unique solution to the IVP.\n",
    "\n",
    "To check that $y_1, y_2$ are linearly independent, we can check either via the definition of linear independence or via the Wronskian. If we want to check via definition, suppose $d_1 y_1(t) + d_2 y_2(t) = 0$ for all $t \\in I$. Note that this implies $d_1 y_1'(t) + d_2 y_2'(t) = 0$ for all $t \\in I$. In particular, we have \n",
    "\\begin{align}\n",
    "d_1 y_1(t_0) + d_2 y_2(t_0) &= 0 \\\\\n",
    "d_1 y_1'(t_0) + d_2 y_2'(t_0) &= 0,\n",
    "\\end{align}\n",
    "or \n",
    "\\begin{align}\n",
    "d_1 &= 0\\\\\n",
    "d_2 &= 0.\n",
    "\\end{align}\n",
    "This shows that $y_1, y_2$ are linearly independent over $I$.\n",
    "\n",
    "We can also verify linear independence directly via the Wronskian. We note that \n",
    "$$\n",
    "W(y_1,y_2)(t_0) = \\det \\begin{pmatrix}\n",
    "y_1(t_0) & y_2(t_0) \\\\\n",
    "y_1'(t_0) & y_2'(t_0)\n",
    "\\end{pmatrix} = \\det \\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix} = 1 \\neq 0.\n",
    "$$\n",
    "Since the Wronskian is non-zero at the point $t_0 \\in I$, the two functions are linearly independent over $I$. $\\square$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that if want to find the general solution to the homogeneous equation, it suffices to identify the $n$ linearly independent \"building blocks.\" This leads to the notion of a **fundamental set of solutions**.\n",
    "\n",
    "> **Definition**\n",
    "> Let $I$ be an interval and let $a_n, \\ldots, a_0: I \\to \\mathbb{R}$ be continuous, and assume $a_n(t) \\neq 0$ for all $t \\in I$. A set of $n$ functions $\\{y_1, \\ldots, y_n\\} \\subseteq C^{n}(I ; \\mathbb{R})$ is called a **fundamental set of solutions** to the $n$-th order homogeneous linear ODE \n",
    "> $$\n",
    "> a_n(t) y^{(n)}(t) + \\ldots + a_1(t) y'(t) + a_0(t)y(t)  = 0, \\; t \\in I\n",
    "> $$\n",
    "> if the functions are linearly independent over $I$ and the general solution to the equation is given by\n",
    "> $$\n",
    "> y_h(t) = c_1 y_1(t) + \\ldots + c_n y_n(t), \\; t \\in I.\n",
    "> $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, identifying the fundamental set of solutions is not easy. However, there is a special case where they can be identified explicitly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant coefficient linear differential equations\n",
    "\n",
    "In the case where the equation in consideration is a homogeneous **constant coefficient** linear differential equation of the form \n",
    "$$\n",
    "a_n y^{(n)} + \\ldots + a_1 y' + a_0 y = 0, \n",
    "$$\n",
    "where $a_n \\neq 0$, the general solution is given by \n",
    "$$y_h(t) = c_1 y_1(t) + \\ldots + c_n y_n(t), t \\in \\mathbb{R},$$\n",
    "and the linearly independent functions $y_1, \\ldots, y_n$ can be identified through the **characteristic equation**\n",
    "$$a_n r^n + \\ldots + a_1 r + a_0 = 0.$$\n",
    "\n",
    "By the fundamental theorem of algebra we know that there are exactly $n$ (possibly complex) roots to this polynomial. Suppose $\\lambda_1, \\lambda_2, ..., \\lambda_n$ are the roots to this polynomial.\n",
    "\n",
    "1. If $\\lambda_1, ..., \\lambda_n$ are all real and they are all distinct, then the solution to \n",
    "$$\n",
    "y(t) = c_1 e^{\\lambda_1 t} + c_2 e^{\\lambda_2 t} + ... + c_n e^{\\lambda_n t}.\n",
    "$$\n",
    "2. If we have $k$ repeated real roots, say $\\lambda_1 = \\lambda_2 = ... = \\lambda_k$ (and $\\lambda_{k+1}, ..., \\lambda_n$ are real distinct roots), then \n",
    "$$\n",
    "y(t) = c_1 \\textcolor{red}{e^{\\lambda_1 t}} + c_2 \\textcolor{red}{te^{\\lambda_2 t}} + c_3 \\textcolor{red}{t^2 e^{\\lambda_3 t}}  ... + c_k \\textcolor{red}{t^{k-1} e^{\\lambda_k t}}  + c_{k+1} e^{\\lambda_{k+1} t} + ... + c_n e^{\\lambda_n t}.\n",
    "$$\n",
    "3. If $\\lambda_i = \\alpha + \\beta i$ is a complex root of multiplicity 1, then $\\lambda_j = \\overline{\\lambda_i} = \\alpha - \\beta i$ must also be a root. Then we replace $c_i e^{\\lambda_i t} + c_j e^{\\lambda_j t}$ (which is still valid, but we are interested only in real-valued solutions) with $c_i e^{\\alpha t} \\cos(\\beta t) + c_j e^{\\alpha t} \\sin(\\beta t)$. \n",
    "4. If $\\lambda = \\alpha + \\beta i$ is a complex root of multiplicity $k$, then its conjugate $\\overline{\\lambda} = \\alpha - \\beta i$ must also be a root of multiplicity $k$. In this case the solution is the same as the solution given in case (2), except we replace the complex exponentials in the previous equation with sines and cosines, as in case (3).\n",
    "\n",
    "This is not to difficult to justify using the aforementioned linear algebraic framework. For now, we will simply verify this through examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "> **Example:**\n",
    "> Consider the IVP\n",
    "> $$\n",
    "> \\begin{cases}\n",
    "> y''(t) + t y'(t) + t^2 y(t) = 0, \\; t \\in I \\\\\n",
    "> y(t_0) = a_1 \\\\\n",
    "> y'(t_0) =a_2.\t\n",
    "> \\end{cases}\n",
    "> $$\n",
    "> Recall that by the existence and uniqueness theorem for higher order equations, as long as the coefficients are continuous and the leading coefficient does not vanish on an interval $I$, there will always exist a unique solution to this IVP that is defined over $I$, for all values of $t_0,a_1, a_2$.\n",
    "> Note that since the equation is linear, the general solution is still given by \n",
    "> $$\n",
    "> y(t) = c_1 y_1(t) + c_2 y_2(t), \\; t \\in I\n",
    "> $$\n",
    "> but since the coefficients are not constant functions, there's no simple mechanism to identify $y_1, y_2$.\n",
    "\n",
    "> **Example:**\n",
    "> Consider the IVP\n",
    "> $$\n",
    "> \\begin{cases}\n",
    "> y'' - 3y'(t) + 2 y(t) = 0, \\;t \\in \\mathbb{R}\n",
    "> y(0) = a\\\\\n",
    "> y'(0) = b.\n",
    "> \\end{cases}\n",
    "> $$\n",
    "> The associated characteristic polynomial is \n",
    "> $$\n",
    "> r^2 - 3r + 2 = (r-1)(r-2) = 0,\n",
    "> $$\n",
    "> so its roots are $r = 1, r= 2$. This means that the general solution to the equation is given by \n",
    "> $$\n",
    "> y(t) = c_1 e^{t} + c_2 e^{2t}, \\; t \\in \\mathbb{R}.\n",
    "> $$\n",
    "> To identify the (unique) solution to the IVP, we need to identify the coefficients $c_1, c_2$ using the initial conditions. We have \n",
    "> $$\n",
    "> y(0) = c_1 e^{0} + c_2 e^{0} = c_1 + c_2 \\\\\n",
    "> y'(0) = c_1 e^{0} + 2 c_2 e^{0} = c_1 + 2c_2,\n",
    "> $$\n",
    "> so for $y$ to be a solution to the IVP we must have \n",
    "> $$\n",
    "> c_1 + c_2 = a\\\\\n",
    "> c_1 + 2c_2 = b.\n",
    "> $$\n",
    "> This implies that $c_2 = b-a, c_1 = 2a-b$. So the unique solution to the IVP is \n",
    "> $$\n",
    "> y(t) = (2a-b) e^t + (b-a) e^{2t}, \\; t \\in \\R.\n",
    "> $$\n",
    "> Notice that a unique solution exists for all values of $a,b$. This is in line with the existence and uniqueness theorem for higher order equations, since the coefficients are constant functions (hence continuous).  \n",
    "\n",
    "> **Example:**\n",
    "> Consider the equation \n",
    "> $$\n",
    "> y''(t) - y(t) = 0, \\; t \\in \\mathbb{R}\n",
    "> $$\n",
    "> The associated characteristic polynomial is \n",
    "> $$\n",
    "> r^2 - 1 =(r-1)(r+1) = 0.\n",
    "> $$\n",
    "> So the general solution to the equation is given by \n",
    "> $$\n",
    "> y(t) = c_1 e^{-t} + c_2 e^{t}, \\; t \\in \\R.\n",
    "> $$\n",
    "> where $c_1, c_2$ are arbitrary constants. \n",
    "> \n",
    "> Notice that we can also write the general solution as\n",
    "> $$\n",
    "> y(t) = d_1 (e^{-t} + e^{t}) + d_2 e^t, \\; t \\in \\R\n",
    "> $$\n",
    "> where $d_1, d_2$ are arbitrary constants, since\n",
    "> $$\n",
    "> d_1 (e^{-t} + e^{t}) + d_2 e^t = d_1 e^{-t} + (d_1 + d_2) e^t, \\; t \\in \\R\n",
    "> $$\n",
    "> and \n",
    "> $$\n",
    "> c_1 e^{-t} + c_2 e^{t} = c_1 (e^{-t} + e^{t}) + (c_2 - c_1) e^{t}, \\; t \\in \\R.\n",
    "> $$\n",
    "> In general, the \"building blocks\" of the general solution are not unique. What's important is that they are solutions to the homogeneous equation and that they are linearly independent. \n",
    "\n",
    "> **Example:**\n",
    "> Consider the equation \n",
    "> $$\n",
    "> y''(t) - 2y'(t) + y(t) = 0, \\; t \\in \\R.\n",
    "> $$\n",
    "> The associated characteristic polynomial is \n",
    "> $$\n",
    "> r^2 - 2r + 1 =(r-1)^2 = 0.\n",
    "> $$\n",
    "> Here, $r = 1$ is a repeated root. So the general solution to the equation is \n",
    "> $$\n",
    "> y(t) = c_1 e^t + c_2 t e^t, \\; t \\in \\R.\n",
    "> $$\n",
    "\n",
    "> **Example**\n",
    "> Consider the equation \n",
    "> $$\n",
    "> y^{(3)} + 3 y'' + 3 y' + y = 0, \\; \\; t \\in \\R.\n",
    "> $$\n",
    "> The associated characteristic polynomial is \n",
    "> $$\n",
    "> r^3 +3r^2 + 3r + 1 =(r+1)^3 = 0.\n",
    "> $$\n",
    "> Here, $r = -1$ is a repeated root of multiplicity 3. So the general solution to the equation is \n",
    "> $$\n",
    "> y(t) = c_1 e^{-t} + c_2 t e^{-t} + c_3 t^2 e^{-t}, \\; \\; t \\in \\R.\n",
    "> $$\n",
    "\n",
    "> **Example:**\n",
    "> Consider the equation\n",
    "> $$\n",
    "> y'''(x) - y(x) = 0, \\; x \\in \\R.\n",
    "> $$\n",
    "> The associated characteristic polynomial is \n",
    "> $$\n",
    "> r^3 - 1 =0. \n",
    "> $$\n",
    "> The roots of this polynomial of given by the *3rd roots of unity*:\n",
    "> $$\n",
    "> \\begin{split}\n",
    "> r_1 &= \\exp \\left( \\frac{2\\pi i}{3} *0 \\right) = 1\\\\\n",
    "> r_2 &=  \\exp \\left( \\frac{2\\pi i}{3} * 1 \\right) = \\cos \\left( \\frac{2\\pi}{3} \\right) + i \\sin \\left( \\frac{2\\pi}{3} \\right) = -\\frac{1}{2} + i \\frac{\\sqrt{3}}{2}. \\\\\n",
    "> r_3 &=  \\exp \\left( \\frac{2\\pi i}{3} *2 \\right) = \\cos \\left( \\frac{4\\pi}{3} \\right) + i \\sin \\left( \\frac{4\\pi}{3} \\right) = -\\frac{1}{2} - i \\frac{\\sqrt{3}}{2}.\n",
    "> \\end{split}\n",
    "> $$\n",
    "> Therefore the general solution is given by \n",
    "> $$\n",
    "> y(x) = c_1e^{x} + c_2 e^{-1/2x} \\cos \\left( \\frac{\\sqrt{3}}{2} x\\right) + c_3 e^{-1/2x} \\sin \\left( \\frac{\\sqrt{3}}{2} x\\right), \\; x \\in \\R.\n",
    "> $$\n",
    "> Another way to find the roots to the characteristic polynomial is by using the factorization\n",
    "> $$\n",
    "> r^3 - 1 = (r-1) (r^2 + r + 1 ).\n",
    "> $$\n",
    "\n",
    "> **Example**\n",
    "> Consider the equation\n",
    "> $$\n",
    "> {x}'''(t) - x''(t) - 4x(t) = 0, \\; t \\in \\R.\n",
    "> $$\n",
    "> The associated characteristic polynomial is \n",
    "> $$\n",
    "> r^3 - r^2 - 4 = 0.\n",
    "> $$\n",
    "> By inspection, $r = 2$ is a root to this polynomial. Then via long division we see that \n",
    "> $$\n",
    "> r^3 - r^2 - 4 = (r-2) (r^2 + r + 2) =0.\n",
    "> $$\n",
    "> Therefore the roots of this polynomial are \n",
    "> $$\n",
    "> \\begin{split}\n",
    "> r_1 &= 2\\\\\n",
    "> r_2 &= \\frac{-1 - \\sqrt{1 - 8}}{2} = -\\frac{1}{2} - i \\frac{\\sqrt{7}}{2}\\\\\n",
    "> r_3 &= -\\frac{1}{2} + i \\frac{\\sqrt{7}}{2}.\n",
    "> \\end{split}\n",
    "> $$\n",
    "> So the general solution is given by \n",
    "> $$\n",
    "> x(t) = c_1e^{2t} + c_2 e^{-1/2t} \\cos \\left( \\frac{\\sqrt{7}}{2} t\\right) + c_3 e^{-1/2t} \\sin \\left( \\frac{\\sqrt{7}}{2} t\\right), \\; t \\in \\R.\n",
    "> $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: complex numbers\n",
    "> **Definition**\n",
    "> A *complex number* is a number of the form $z = a+ib$, where $a,b$ are real numbers and $i$ is the imaginary unit satisfying $i^2 = -1$. $a$ is referred to as the *real part* of $z$ and denoted as $\\Re(z)$. $b$ is referred to as the *imaginary part* of $z$ and denoted as $\\Im(z)$. \n",
    "\n",
    "We usually denote the set of complex numbers with $\\mathbb{C}$.\n",
    "\n",
    "> **Definition**\n",
    "> The *complex conjugate* of a complex number $a + ib$ is the complex number $a - ib$. We often denote the complex conjugate of a complex number $z$ by $\\overline{z}$.\n",
    "\n",
    "> **Example**\n",
    "> The complex conjugate of $3 + 2i$ is \n",
    "> $$\n",
    "> \\overline{3+2i} = 3-2i.\n",
    "> $$\n",
    "\n",
    "> **Theorem (Fundamental theorem of algebra)**\n",
    "> Let $p(z)$ be an $n$-th degree polynomial with possibly complex coefficients. Then $p$ has (up to multiplicity) exactly $n$ roots.\n",
    "\n",
    "> **Proposition**\n",
    "> If a complex number $z = a+ib$ is a root to a polynomial $p(z)$, then its conjugate $\\overline{z}$ is also a root of $p(z)$.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
