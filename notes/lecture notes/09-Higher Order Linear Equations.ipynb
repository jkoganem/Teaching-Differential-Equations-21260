{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Higher Order Linear Equations**\n",
    "\n",
    "---\n",
    "\n",
    "### **Introduction**\n",
    "\n",
    "This notebook introduces concepts and techniques useful for studying higher order equations. \n",
    "\n",
    "---\n",
    "\n",
    "### **Author**\n",
    "**Junichi Koganemaru**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Last Updated**\n",
    "**January 28, 2025**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we aim to study general $n$-th order linear differential equations of the form\n",
    "\n",
    "$$\n",
    "a_n(t) y^{(n)}(t) + a_{n-1}(t) y^{(n-1)}(t) + \\cdots + a_1(t) y'(t) + a_0(t) y(t) = f(t), \\; t \\in \\mathbb{R}\n",
    "$$\n",
    "where $a_0, ..., a_n, f: \\mathbb{R} \\to \\mathbb{R}$ are assumed to be continuous. \n",
    "\n",
    "The focus for us will be the case when $n = 2$, as many physical systems are modeled via second order differential equations. However, we will see that the theory for higher order equations is completely analogous.\n",
    "\n",
    "First, we go over an example to illustrate why the concepts we introduce in the next section are useful.\n",
    "\n",
    "> **Example:**\n",
    "> Consider the second order linear differential equation\n",
    "> $$\n",
    "> y''(t) + 2y'(t) + 2y(t) = f(t), \\; t \\in \\mathbb{R},\n",
    "> $$\n",
    "> where $f$ is assumed to be continuous. If $y: \\mathbb{R} \\to \\mathbb{R}$ is a solution to the equation above, consider the **vector-valued function** $\\boldsymbol{Y}: \\mathbb{R} \\to \\mathbb{R}^2$ defined via\n",
    "> $$\n",
    "> \\boldsymbol{Y}(t) = \\begin{pmatrix} y(t) \\\\ y'(t) \\end{pmatrix}.\n",
    "> $$\n",
    "> Here $Y$ takes in a value $t \\in \\mathbb{R}$ and gives back a **column vector** in $\\mathbb{R}^2$. We will carefully introduce operations that can be performed on vector-valued functions later, for now we just need to know how to differentiate them. For vector valued functions, their derivatives are defined **component wise**, i.e. \n",
    "> $$\n",
    "> \\boldsymbol{Y}'(t) = \\begin{pmatrix} \\frac{d}{dt} [y(t)] \\\\ \\frac{d}{dt} [y'(t)] \\end{pmatrix} = \\begin{pmatrix} y'(t) \\\\ y''(t) \\end{pmatrix}, \\; t \\in \\mathbb{R}.\n",
    "> $$\n",
    "> Note that since $y$ is a solution to the differential equation, we have that $y''(t) = -2y'(t) - 2y(t) + f(t)$. Therefore, we have \n",
    "> $$\n",
    "> \\boldsymbol{Y}'(t) = \\begin{pmatrix} y'(t) \\\\ y''(t) \\end{pmatrix} = \\begin{pmatrix} y'(t) \\\\ -2y'(t) - 2y(t) + f(t) \\end{pmatrix} =  \\begin{pmatrix} 0 \\cdot y(t) + 1 \\cdot y'(t) \\\\ - 2 \\cdot y(t) -2 \\cdot y'(t)  \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ f(t) \\end{pmatrix}.\n",
    "> $$\n",
    "> Note that the first term on the right hand side only depends on the entries of $\\boldsymbol{Y}$. Using some notation that we will go over later, we can write the above equation as\n",
    "> $$\n",
    "> \\boldsymbol{Y}'(t) =  \\begin{pmatrix}  0 & 1 \\\\ -2 & -2 \\end{pmatrix} \\begin{pmatrix} y(t) \\\\ y'(t) \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ f(t) \\end{pmatrix},\n",
    "> $$\n",
    "> where \n",
    "> $$ \n",
    "> A = \\begin{pmatrix}  0 & 1 \\\\ -2 & -2 \\end{pmatrix}\n",
    "> $$\n",
    "> is a **matrix** recording the coefficients of the components of $\\boldsymbol{Y}$. We thus obtain a **first order vector-valued** equation  \n",
    "> $$\n",
    "> \\boldsymbol{Y}'(t) = A \\boldsymbol{Y}(t) + \\boldsymbol{F}(t) \\Longleftrightarrow \\boldsymbol{Y}'(t) - A  \\boldsymbol{Y}(t) =\\boldsymbol{F}(t), \\; t \\in \\mathbb{R},\n",
    "> $$\n",
    "> where $F: \\mathbb{R} \\to \\mathbb{R}^2$ is defined via $\\boldsymbol{F}(t) = \\begin{pmatrix} 0 \\\\ f(t) \\end{pmatrix}$. This is a **first order linear differential equation** for the vector-valued function $\\boldsymbol{Y}$.\n",
    "\n",
    "The upshot of this example is that we can rewrite a second order scalar differential equation as a first order vector-valued differential equation. \n",
    "\n",
    "In fact, this can be done for general $n$-th order linear equations as well, which means that we can always trade the order of the differential equation for the number of components of the vector-valued function $\\boldsymbol{Y}$. \n",
    "\n",
    "While this is extremely powerful, as it shows that any $n$-th order scalar equation can be studied as a first order vector-valued equation, it requires one to be comfortable with the language of linear algebra. We will take this approach later as it gives us a unified framework for studying linear equations.\n",
    "\n",
    "For now, we will only study second order equations as scalar equations, however the example shows that the theory of $n$-th order linear equations must be intricately connected to concepts coming from linear algebra. We will introduce some preliminary notions below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries \n",
    "\n",
    "### Linear Independence\n",
    "Linear independence is a notion from linear algebra that encodes the \"dependencies\" among a collection of objects.\n",
    "\n",
    "It is then useful in this discussion to borrow some terminology from linear algebra.\n",
    "\n",
    "> **Definition (Linear combination of functions)**\n",
    "> Given $n$ continuous functions $y_1, \\ldots, y_n: I \\to \\mathbb{R}$ on an interval $I$, a linear combination of these functions is another continuous function $y: I \\to \\mathbb{R}$ which can be written as \n",
    "> $$\n",
    "> y(t) = c_1 y_1(t) + c_2 y_2(t) + \\ldots + c_n y_n(t), \\; t \\in I\n",
    "> $$\n",
    "> where $c_1, \\ldots, c_n \\in \\mathbb{R}$ are constants.\n",
    "\n",
    "To motivate our discussion, let's first consider an example.\n",
    "\n",
    "> **Example**\n",
    "> Consider the functions $y_1, y_2, y_3: \\mathbb{R} \\to \\mathbb{R}$ defined via $y_1(t) = t, y_2(t) = t+1, y_3(t) = 2t+1$ for $t \\in \\mathbb{R}$, and suppose that $y$ is a linear combination of $y_1, y_2, y_3$: \n",
    "> $$\n",
    "> y(t) = c_1 y_1(t) + c_2 y_2(t) + c_3 y_3(t), \\; t \\in \\mathbb{R} \n",
    "> $$\n",
    "> for some constants $c_1, c_2, c_3$. Notice that here we have \n",
    "> $$\n",
    "> y_1 + y_2 = y_3,\n",
    "> $$\n",
    "> so in the equation above, we can replace $y_3$ and write \n",
    "> $$\n",
    "> y(t) = c_1 y_1(t) + c_2 y_2(t) + c_3(y_1(t) + y_2(t)) = C_1 y_1(t) + C_2 y_2(t), \\; t \\in \\mathbb{R}\n",
    "> $$\n",
    "> where $C_1 = c_1 + c_3, C_2 = c_2 + c_3$.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  What this shows us is that because $y_3$ is \"dependent\" on $y_1$ and $y_2$, even though in the definition of $y$ there seems to be three \"building blocks\", in reality only two \"building blocks\" are required to create $y$. In other words, any linear combination of $y_1, y_2, y_3$ can always be written as a linear combination of $y_1, y_2$.\n",
    "  \n",
    "  In this sense, the appearance of $y_3$ is redundant.\n",
    "\n",
    "We will see later that general solutions to homogeneous linear differential equation is given as a linear combination of functions. What we want to do is find a way to capture the notion of \"non-redundancy\". This is where the notion of **linear independence** comes in. First, let us give an intuitive definition as to what it means for a set of functions to be linearly independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition (Linear independence I)**\n",
    "> A set of continuous functions defined over an interval $I$ is said to be *linearly independent* if none of the functions can be written as the linear combination of the other functions in the set, on the interval $I$.\n",
    "\n",
    "In the previous example, the set $\\{y_1, y_2, y_3\\}$ is not linearly independent over $\\mathbb{R}$ because $y_3$ is a linear combination of $y_1$ and $y_2$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous definition, while conceptually clear, is hard to apply in practice. Instead we will often use the following alternative definition.\n",
    "\n",
    "> **Definition (Linear independence II)**\n",
    "> A set of continuous functions $y_1, \\ldots, y_n: I \\to \\mathbb{R}$ is said to be linearly independent on an interval $I$ if \n",
    "> $$\n",
    "> c_1 y_1(t) + \\ldots + c_n y_n(t) = 0 \\; \\text{for all} \\; t \\in I\n",
    "> $$\n",
    "> implies $c_1 = c_2 = \\ldots = c_n = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see why the two definitions are equivalent to each other. Suppose there exists some non-zero $c_i$'s such that \n",
    "$$\n",
    "c_1 y_1(t) + \\ldots + c_k y_k(t)= 0, \\; t \\in I.\n",
    "$$\n",
    "Up to relabeling of the indices, we can suppose without loss of generality that $c_1 \\neq 0$. Then we can immediately write \n",
    "$$\n",
    "y_1(t) = -\\frac{1}{c_1} \\left( c_2 y_2(t) + \\ldots + c_k y_k(t) \\right), \\; t \\in I\n",
    "$$\n",
    "meaning that $y_1$ is a linear combination of other functions in the set. On the other hand, if a function is already a linear combination of the other functions, then we can find non-zero coefficients to produce the zero function. So the two definitions are equivalent to each other.\n",
    "\n",
    "**Remark:** According to this definition, any set containing the zero function is automatically linearly dependent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example**\n",
    "> Consider the functions $y_1, y_2, y_3$ defined via $y_1(t) = e^t, y_2(t) = e^{-t}, y_3(t) = e^{t} + e^{-t}$ for $t \\in \\mathbb{R}$. Notice that\n",
    "> $$\n",
    "> y_3(t) = y_1(t) + y_2(t) \\Longleftrightarrow y_1(t) + y_2(t) - y_3(t) = 0, \\; t \\in \\mathbb{R}\n",
    "> $$\n",
    "> for all real values of $t$. So if $c_1 y_1(t) + c_2 y_2(t) + c_3 y_3(t) = 0$ for all $t \\in \\mathbb{R}$, it's not necessarily true that $c_1 = c_2 = c_3 = 0$. Therefore these three functions are linearly dependent.\n",
    "\n",
    "> **Example**\n",
    "> Consider the functions $y_1, y_2, y_3$ defined via $y_1(t) = \\sin^2(t), y_2(t) = \\cos^2(t), y_3(t) = 1$ for $t \\in \\mathbb{R}$. Since \n",
    "> $$\n",
    "> y_1(t) + y_2(t) = \\sin^2(t) + \\cos^2(t) = 1 = y_3(t)\n",
    "> $$\n",
    "> for all $t \\in \\mathbb{R}$, we see that these functions are linearly dependent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wronskian\n",
    "\n",
    " In practice, it is usually cumbersome to check linear independence via either of the previous definition when the set contains more than two functions. So instead, we'll again borrow some ideas from linear algebra and encode the dependence/independence of these functions in a single object - the Wronskian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Definition**\n",
    "> Let $I \\subseteq \\mathbb{R}$ be an interval and let $y_1, ..., y_n \\in C^{n-1}(I ; \\mathbb{R})$. Then the *Wronskian* of this collection of functions is a function $W: I \\to \\mathbb{R}$ defined as a **determinant** of a matrix,\n",
    "> $$\n",
    "> W(y_1, y_2, ..., y_n) (t) := \\det \\begin{pmatrix}\n",
    "> y_1(t) & y_2(t) & \\cdots & y_n(t) \\\\\n",
    "> y_1'(t) & y_2'(t) & \\cdots & y_n'(t) \\\\\n",
    "> \\vdots & \\vdots & \\ddots &\\vdots  \\\\\n",
    "> y_1^{(n-1)}(t) & y_2^{(n-1)}(t) & \\cdots & y_n^{(n-1)}(t) \n",
    "> \\end{pmatrix}, \\; t \\in I.\n",
    "> $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following proposition establishes a connection between the Wronskian and linear independence.\n",
    "\n",
    "> **Proposition**\n",
    "> Let $y_1, \\ldots , y_n: I \\to \\mathbb{R}$ be $n$ **analytic functions** over an interval $I$. Then the set of functions are linearly dependent if and only if $W(y_1,\\ldots ,y_n)$ is identically zero on $I$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words,\n",
    "1. If the Wronskian of $n$ analytic functions is identically zero over an interval $I$, then the set of functions is linearly dependent.\n",
    "2. Otherwise, the set of functions is linearly independent.\n",
    "\n",
    "This means as long as we can compute the Wronksian of a set of analytic functions, we can determine whether they are linearly independent or not.\n",
    "\n",
    "**Remark:** Analytic functions are smooth functions that admit a local power series representation at every point in their respective domains. In practice, many functions that we encounter are analytic, so this theorem is quite useful. Constant functions, trigonometric functions, polynomials, and exponential functions are all examples of analytic functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the functions in question are solutions to linear differential equation, we can say something even stronger. \n",
    "\n",
    "> **Theorem (Cauchyâ€“Kovalevskaya)** \n",
    "> Let $y_1, ..., y_n \\in C^{n-1}(\\mathbb{R})$ be $n$ solutions to an $n$-th order homogeneous linear differential equation on an interval $I$ of the form \n",
    "> $$\n",
    "> y^{(n)}(t) + a_{n-1}(t) y^{(n-1)}(t) + \\ldots  + a_1(t)y'(t) + a_0(t) y(t) = 0, \\; t \\in I\n",
    "> $$\n",
    "> where $\\{a_i\\}_{i=1}^{n-1}$ are *analytic* over $I$. Then $y_1, ..., y_n$ are analytic and the set of solutions is linearly dependent if and only if $W(y_1, y_2, ..., y_n)(t) = 0$ for some $t \\in I$. \n",
    "\n",
    "**Remark**: This theorem is stated only for functions that are solutions to homogeneous equations of a specific form (leading coefficient must be 1 and the coefficients must be analytic), which is why in the statement we only need that the Wronskian is zero at one point instead of the interval. It turns out that in this situation, vanishing at one point immediately implies that the Wronskian is identically zero on the entire interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinants \n",
    "Next we discuss how to compute the Wronskian by going over how to compute determinants. In this class we will only introduce how to compute determinants of $2 \\times 2$ and $3 \\times 3$ matrices.\n",
    "\n",
    "### Determinants of $2 \\times 2$ matrices\n",
    "\n",
    "We first consider the determinant of $2 \\times 2$ matrices.\n",
    "\n",
    "> **Definition**  \n",
    "> Consider the square matrix  \n",
    "> $$  \n",
    "> A = \\begin{pmatrix}  \n",
    "> a & b \\\\  \n",
    "> c & d  \n",
    "> \\end{pmatrix}.  \n",
    "> $$  \n",
    "> The *determinant* of $A$ is a number associated to the matrix $A$, defined by  \n",
    "> $$  \n",
    "> \\det A := ad - bc.  \n",
    "> $$  \n",
    "\n",
    "**Remark** \n",
    "The determinant is sometimes denoted with vertical bars: \n",
    "$$\n",
    "\\det(A) = \\begin{vmatrix}\n",
    "a & b \\\\\n",
    "c & d\n",
    "\\end{vmatrix}.\n",
    "$$\n",
    "\n",
    "> **Example:**\n",
    "> Let \n",
    "> $$\n",
    "> A = \\begin{pmatrix} \n",
    "> 1 & 2 \\\\\n",
    "> 3 & 4 \n",
    "> \\end{pmatrix}, B = \\begin{pmatrix} \n",
    "> 2 & 3 \\\\\n",
    "> 4 & 5\n",
    "> \\end{pmatrix}. \n",
    "> $$\n",
    "> Then\n",
    "> $$\n",
    "> \\det A = 1 \\times 4 - 2 \\times 3 = -2, \\det B = 2 \\times 5 - 3 \\times 4 = -2.\n",
    "> $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Determinants of $3 \\times 3$ matrices\n",
    "\n",
    "Suppose $A$ is a $3 \\times 3$ matrix of the form\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "The simplest way to calculate $\\det(A)$ is to use\n",
    "$$\n",
    "\\det(A) = \\begin{vmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{vmatrix} = a  \\begin{vmatrix}\n",
    "e & f \\\\\n",
    "h & i\n",
    "\\end{vmatrix} - d \\begin{vmatrix}\n",
    "b & c \\\\\n",
    "h & i\n",
    "\\end{vmatrix}  + g\\begin{vmatrix}\n",
    "b & c \\\\\n",
    "e & f\n",
    "\\end{vmatrix}\n",
    "= a(ei - fh) - d(bi - ch) + g( bf - ce).\n",
    "$$\n",
    "\n",
    "One way to interpret this is that we are expanding along the first column, and we're multiplying each element in the column by the determinant of the matrix obtained from ignoring the row and column containing that element. Here's the way to visualize it (the smaller matrices are called \\emph{minors})\n",
    "$$\n",
    "\\begin{vmatrix}\n",
    "a & \\cdots & \\cdots \\\\\n",
    "\\vdots & \\textcolor{red}{e} & \\textcolor{red}{f}\\\\\n",
    "\\vdots & \\textcolor{red}{h} & \\textcolor{red}{i}\n",
    "\\end{vmatrix}  \\quad \\quad \\begin{vmatrix}\n",
    "\\vdots & \\textcolor{red}{b} & \\textcolor{red}{c}\\\\\n",
    "d & \\cdots & \\cdots\\\\\n",
    "\\vdots & \\textcolor{red}{h} & \\textcolor{red}{i}\n",
    "\\end{vmatrix} \\quad \\quad \n",
    "\\begin{vmatrix}\n",
    "\\vdots & \\textcolor{red}{b} & \\textcolor{red}{c}\\\\\n",
    "\\vdots & \\textcolor{red}{e} & \\textcolor{red}{f}\\\\\n",
    "g & \\cdots & \\cdots\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "Notice that the signs in the expansion flipped from $+a$ to $-d$ to $+g$. This is important. \n",
    "\n",
    "In general one can expand along any row and any column, as long as you have the right sign in front of the elements in the expansion. The signs associated with each element is given in the matrix on the right:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{pmatrix} \\quad \\quad \\begin{pmatrix}\n",
    "+ & - & +\\\\\n",
    "- & +& -\\\\\n",
    "+ & - & +\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So in the equation above\n",
    "$$\n",
    "\\det(A) = \\textcolor{red}{+a}  \\begin{vmatrix}\n",
    "e & f \\\\\n",
    "h & i\n",
    "\\end{vmatrix} \\textcolor{red}{-d} \\begin{vmatrix}\n",
    "b & c \\\\\n",
    "h & i\n",
    "\\end{vmatrix}  \\textcolor{red}{+g} \\begin{vmatrix}\n",
    "b & c \\\\\n",
    "e & f\n",
    "\\end{vmatrix},\n",
    "$$\n",
    "we took\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\textcolor{red}{a} & b & c\\\\\n",
    "\\textcolor{red}{d} & e & f\\\\\n",
    "\\textcolor{red}{g} & h & i\n",
    "\\end{pmatrix} \\quad \\quad \\begin{pmatrix}\n",
    "\\textcolor{red}{+} & - & +\\\\\n",
    "\\textcolor{red}{-} & +& -\\\\\n",
    "\\textcolor{red}{+} & - & +\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "If we were to expand along the second column then we'd have the minors as \n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "\\cdots & b & \\cdots \\\\\n",
    "\\textcolor{blue}{d} & \\vdots  & \\textcolor{blue}{f}\\\\\n",
    "\\textcolor{blue}{g} & \\vdots & \\textcolor{blue}{i}\n",
    "\\end{pmatrix}  \\quad \\quad \\begin{pmatrix}\n",
    "\\textcolor{blue}{a} & \\vdots &  \\textcolor{blue}{c} \\\\\n",
    "\\cdots & e  & \\cdots\\\\\n",
    "\\textcolor{blue}{g} & \\vdots & \\textcolor{blue}{i}\n",
    "\\end{pmatrix} \\quad \\quad \n",
    "\\begin{pmatrix}\n",
    "\\textcolor{blue}{a} & \\vdots &  \\textcolor{blue}{c} \\\\\n",
    "\t\\textcolor{blue}{g} & \\vdots & \\textcolor{blue}{i}\n",
    "   \\end{pmatrix}  \\quad \\quad \\begin{pmatrix}\n",
    "\t\\textcolor{blue}{a} & \\vdots &  \\textcolor{blue}{c} \\\\\n",
    "\t\\cdots & e  & \\cdots\\\\\n",
    "\t\\textcolor{blue}{g} & \\vdots & \\textcolor{blue}{i}\n",
    "   \\end{pmatrix} \\quad \\quad \n",
    "   \\begin{pmatrix}\n",
    "   \\textcolor{blue}{a} & \\vdots &  \\textcolor{blue}{c} \\\\\n",
    "\t\\textcolor{blue}{d} & \\vdots  & \\textcolor{blue}{f}\\\\\n",
    "   \\cdots& h & \\cdots\n",
    "   \\end{pmatrix}\n",
    "   $$\n",
    "   and thus \n",
    "   $$\n",
    "   \\det(A) =  \\textcolor{red}{-b}  \\begin{vmatrix}\n",
    "   d & f \\\\\n",
    "   g & i\n",
    "   \\end{vmatrix}  \\textcolor{red}{+e} \\begin{vmatrix}\n",
    "   a & c \\\\\n",
    "   g & i\n",
    "   \\end{vmatrix} \\textcolor{red}{-h} \\begin{vmatrix}\n",
    "   a & c \\\\\n",
    "   d & f\n",
    "   \\end{vmatrix}.\n",
    "   $$\n",
    "\n",
    "> **Example:**\n",
    "> Let \n",
    "> $$\n",
    "> A = \\begin{pmatrix} \n",
    "> 1 & 2 & 3 \\\\\n",
    "> 4 & 5 & 6 \\\\\n",
    "> 7 & 8 & 9\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> According to the discussion above, we can evaluate $\\det$ along the second row:\n",
    "> $$\n",
    "> \\det A = -4 \\times \\det \\begin{pmatrix} \n",
    "> 2 & 3 \\\\\n",
    "> 8 & 9\n",
    "> \\end{pmatrix} + 5 \\times \\det \\begin{pmatrix} \n",
    "> 1 & 3 \\\\\n",
    "> 7 & 9\n",
    "> \\end{pmatrix} - 6 \\times  \\det \\begin{pmatrix} \n",
    "> 1 & 2 \\\\\n",
    "> 7 & 8\n",
    "> \\end{pmatrix}.\n",
    "> $$\n",
    "> We can also evaluate along the second column:\n",
    "> $$\n",
    "> \\det A = -2 \\times \\det \\begin{pmatrix} \n",
    "> 4 & 6 \\\\\n",
    "> 7 & 9\n",
    "> \\end{pmatrix} + 5 \\times \\det  \\begin{pmatrix} \n",
    "> 1 & 3 \\\\\n",
    "> 7 & 9\n",
    "> \\end{pmatrix} - 8 \\times \\det \\begin{pmatrix} \n",
    "> 1 & 3 \\\\\n",
    "> 4 & 6\n",
    "> \\end{pmatrix}.  \n",
    "> $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples calculating Wronskians\n",
    "\n",
    "> **Example**\n",
    "> Consider the functions $y_1, y_2: \\mathbb{R} \\to \\mathbb{R}$ defined via \n",
    "> $$\n",
    "> y_1(x) = e^x, \\; y_2(x) = e^{2x}.\n",
    "> $$\n",
    "> Then \n",
    "> $$\n",
    "> W(y_1,y_2) (x) = \\det \\begin{pmatrix}\n",
    "> e^x & e^{2x} \\\\\n",
    "> e^x & 2e^{2x}\n",
    "> \\end{pmatrix} = 2e^{3x} - e^{3x} = e^{3x} \\neq 0 \\; \\text{for all} \\; x \\in \\mathbb{R}.\n",
    "> $$\n",
    "> Therefore the two functions are linearly independent over any interval $I \\subseteq \\mathbb{R}$.  \n",
    "\n",
    "> **Example**\n",
    "> Consider the functions $y_1, y_2: \\mathbb{R} \\to \\mathbb{R}$ defined via \n",
    "> $$\n",
    "> y_1(t) = t^2, \\; y_2(t) = t^3.\n",
    "> $$\n",
    "> Then \n",
    "> $$\n",
    "> W(y_1,y_2) (t) = \\det \\begin{pmatrix}\n",
    "> t^2 & t^3 \\\\\n",
    "> 2t & 3t^2\n",
    "> \\end{pmatrix} = 3t^4 - 2t^4 = t^4 \\neq 0 \\; \\text{for all} \\; t \\neq 0.\n",
    "> $$\n",
    "> Therefore the two functions are linearly independent on any interval $I \\subseteq \\mathbb{R}$, even if it includes $t = 0$.  \n",
    "\n",
    "**Remark:**\n",
    "Since the Wronskian in the previous example doesn't vanish identically on any interval $I$ containing $0$, this also means that on any interval $I$ containing 0, there is no equation of the form \n",
    "$$\n",
    "y''(t) + a_1(t) y'(t) + a_0(t)y(t) = 0, \\; t \\in I\n",
    "$$\n",
    "with analytic coefficients that has these two functions as solutions, as otherwise either the Wronskian is either never zero or it vanishes identically. \n",
    "\n",
    "However, if $I$ does not contain 0, then the Wronskian never vanishes and this is possible. One can check that $y_1, y_2: (0,\\infty) \\to \\mathbb{R}$ are solutions to \n",
    "$$\n",
    "y''(t) - \\frac{5}{2t} y'(t) + \\frac{3}{2t^2} y(t) = 0, \\; t > 0.\n",
    "$$\n",
    "on the interval $I = (0,\\infty)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Example**\n",
    "> Consider the functions $y_1, y_2, y_3: \\mathbb{R} \\to \\mathbb{R}$ defined via \n",
    "> $$\n",
    "> y_1(t) = \\sin^2(t), \\; y_2(t) = \\cos^2(t), \\; y_3(t) = 1.\n",
    "> $$\n",
    "> Then \n",
    "> $$\n",
    "> W(y_1,y_2, y_3) (t) = \\det \\begin{pmatrix}\n",
    "> \\sin^2(t) & \\cos^2(t) & 1 \\\\\n",
    "> 2 \\sin(t) \\cos(t) & -2 \\sin(t)\\cos(t) & 0 \\\\\n",
    "> 2 \\cos(2t) & -2 \\cos(2t) & 0\n",
    "> \\end{pmatrix}\n",
    "> = \\det \\begin{pmatrix}\n",
    "> 2 \\sin(t) \\cos(t) & -2 \\sin(t)\\cos(t) \\\\\n",
    "> 2 \\cos(2t) & -2 \\cos(2t)\n",
    "> \\end{pmatrix}\n",
    "> = 0 \\; \\text{for all} \\; t \\in \\mathbb{R}.\n",
    "> $$\n",
    "> Therefore the three functions are linearly dependent on any interval $I \\subseteq \\mathbb{R}$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
